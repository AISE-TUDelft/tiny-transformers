{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training [`GPT-Neo`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py) on [`TinyStories`](https://huggingface.co/datasets/roneneldan/TinyStories)\n",
    "\n",
    "This script shows how to pre-train both models. I put them in one notebook because the majority of the code is shared; but you may want to separate the logic per model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch wandb transformers[torch] datasets tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: filip-ignijic (filipignijic). Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "\n",
    "\n",
    "import wandb; wandb.login()\n",
    "\n",
    "# TODO: Some gpu error, this fixes it but ask aral for the details\n",
    "import torch\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "small_dataset = True\n",
    "if len(sys.argv) > 1 and sys.argv[1] and sys.argv[1] == \"true\":\n",
    "    small_dataset = True\n",
    "elif len(sys.argv) > 1 and sys.argv[1] and sys.argv[1] == \"false\":\n",
    "    small_dataset = False\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    GPTNeoForCausalLM, GPTNeoConfig, GPT2TokenizerFast, set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models \n",
    "We consider GPT-Neo and BERT as base transformer architectures. This consists of the following blocks linked via residual connections:\n",
    "\n",
    "- Embeddings: Map one-hot (sparse) token vectors to a dense vector of length `hidden_size`. Add positional encoding. \n",
    "- $n$ Blocks: Contain self-attention and FFNs.\n",
    "- Head: Map hidden state back to a useful output. \n",
    "\n",
    "This specifies some of the model-related hyperparameters. I chose them based on what achieved reasonable performance in the [TinyStories paper](https://arxiv.org/abs/2305.07759), while also being feasible to train on our limited compute budgets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gpt = dict(\n",
    "\n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   # number of tokens in the vocabulary \n",
    "    hidden_size             = 516,      # to get to 10m parameters\n",
    "    max_position_embeddings = 512,      # maximum sequence length (context window)\n",
    "\n",
    "    # BLOCKS (ATTN & FFN)\n",
    "    num_layers=2,                               # number of transformer blocks\n",
    "    attention_types=[[[\"global\", \"local\"], 1]], # (GPT-Neo-specific) global and local attention \n",
    "    num_heads=4,                                # attention heads\n",
    "    window_size=256,                            # (GPT-Neo-specific) for local attention \n",
    "    intermediate_size=1024,                     # size of 'up-projection' layer in FFN\n",
    "\n",
    "    pad_token_id = 0,           # need to specify this for tokenizer interop between models\n",
    ")\n",
    "\n",
    "config_gpt = GPTNeoConfig(**config_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement KAN instead of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kan import KanLayer as BaseKANLayer\n",
    "#import torch\n",
    "#from torch import nn\n",
    "#\n",
    "#'''\n",
    "#class GPTNeoMLP(nn.Module):\n",
    "#    def __init__(self, intermediate_size, config):  # in MLP: intermediate_size= 4 * hidden_size\n",
    "#        super().__init__()\n",
    "#        embed_dim = config.hidden_size\n",
    "#        self.c_fc = nn.Linear(embed_dim, intermediate_size)\n",
    "#        self.c_proj = nn.Linear(intermediate_size, embed_dim)\n",
    "#        self.act = ACT2FN[config.activation_function]\n",
    "#        self.dropout = nn.Dropout(float(config.resid_dropout))\n",
    "#\n",
    "#    def forward(self, hidden_states):\n",
    "#        hidden_states = self.c_fc(hidden_states)\n",
    "#        hidden_states = self.act(hidden_states)\n",
    "#        hidden_states = self.c_proj(hidden_states)\n",
    "#        hidden_states = self.dropout(hidden_states)\n",
    "#        return hidden_states\n",
    "#'''\n",
    "#\n",
    "#'''\n",
    "#self.mlp = GPTNeoMLP(inner_dim, config)\n",
    "#'''\n",
    "#\n",
    "#class KANLayer(nn.Module):\n",
    "#    def __init__(self, config):\n",
    "#        super(KANLayer, self).__init__()\n",
    "#        self.in_dim = config.hidden_size\n",
    "#        self.out_dim = config.hidden_size  # Keeping output size same as input\n",
    "#        self.kan_internal = BaseKANLayer(\n",
    "#            in_dim=self.in_dim,\n",
    "#            out_dim=self.out_dim,\n",
    "#            num=5,  # Example parameter\n",
    "#            k=3,  # Example parameter\n",
    "#            noise_scale=0.1,  # Example parameter\n",
    "#            scale_base=1.0,  # Adjust as necessary\n",
    "#            scale_sp=1.0,  # Adjust as necessary\n",
    "#            base_fun=nn.SiLU(),  # Example activation function used within KAN\n",
    "#            grid_eps=0.02,  # Example parameter\n",
    "#            grid_range=[-1, 1],  # Example parameter range\n",
    "#            sp_trainable=True,\n",
    "#            sb_trainable=True,\n",
    "#            device=config.device\n",
    "#        )\n",
    "#        self.dropout = nn.Dropout(config.resid_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom KAN implementation to GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This GPT has 16,613,136 parameters,\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from kan import KANLayer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomGPTNeoForCausalLM(GPTNeoForCausalLM):\n",
    "    def __init__(self, config, mlp_implementation=None):\n",
    "        super().__init__(config)\n",
    "        self.attn_implementation = mlp_implementation\n",
    "        \n",
    "        if mlp_implementation == \"KAN\":\n",
    "            # Override MLP with KAN in each transformer block\n",
    "            for block in self.transformer.h:\n",
    "                block.mlp = KANLayer(in_dim=config.hidden_size, out_dim=config.hidden_size)\n",
    "set_seed(42)\n",
    "\n",
    "model = CustomGPTNeoForCausalLM(config=config_gpt, mlp_implementation=\"KAN\")\n",
    "\n",
    "print(f'''\n",
    "    This GPT has {model.num_parameters():,} parameters,\n",
    "    ''')\n",
    "\n",
    "# gpt, rob # uncomment to see model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing\n",
    "All experiments use the same tokenizer, so in theory, we only need to preprocess our data once. This means you can subsequently skip this CPU-intensive task on the DelftBlue GPU nodes. \n",
    "(except if you are experimenting with different context lengths, in that case you will need to re-tokenise to recover potentially truncated inputs; see below).\n",
    "\n",
    "When running experiments on delftblue, it may be fastest to load (parts of) the dataset into memory, as the cluster has pretty slow IO. See `datasets`, specifically [`load_dataset`'s `keep_in_memory` attribute](https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm \n",
    "#from datasets import load_dataset, DatasetDict\n",
    "#\n",
    "#dataset = load_dataset('roneneldan/tinystories', num_proc=16)\n",
    "#\n",
    "#n_words = 0 \n",
    "#for story in tqdm.tqdm(dataset['train']['text']):    # tqdm is used for progress bars around iterables\n",
    "#    n_words += len(story.split())\n",
    "#\n",
    "#print(f'''\n",
    "#    Train dataset has {len(dataset['train']['text']):,} samples, \n",
    "#    totalling {n_words:,} words (avg. {n_words/len(dataset['train']['text']):,.1f} per story).\n",
    "#    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation\n",
    "Two things happen during tokenisation, of which the latter is optional:\n",
    "\n",
    "1. Map (sub-)words to integers $ \\in [0, 10000]$.\n",
    "2. Truncate/pad each input as necessary to fit within `hidden_size`. \n",
    "\n",
    "All it does is map frequent character strings (i.e. tokens) to integers, which it can then use to look up the corresponding embedding vector for that integer index.\n",
    "\n",
    "The intuition is that by assigning each token its own embedding (vector of `1 x hidden_size`) in the transformer, will help it learn semantical understanding of these tokens. E.g. say that you have two tokens 'about' and 'around', the transformer should learn similar embedding vectors for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "def get_tokenizer_for_config(Tok: PreTrainedTokenizer, config: PretrainedConfig):\n",
    "\n",
    "    tokenizer = Tok.from_pretrained(\n",
    "        '10k-tok',                                         # our custom tokenizer\n",
    "        model_max_length=config.max_position_embeddings    # sequence length (context window)\n",
    "    )\n",
    "\n",
    "    # we're using our special tokenizer with only 10'000 tokens instead of 50'256\n",
    "    assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "    print(f'padding token is {tokenizer.pad_token}')\n",
    "    print(f'padding token in config: {config.pad_token_id}, in tokeniser: {tokenizer.pad_token_id}')\n",
    "    \n",
    "    return tokenizer \n",
    "\n",
    "tok_gpt = get_tokenizer_for_config(GPT2TokenizerFast, config_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment to tokenize the dataset (both tokenizers should give identical results)\n",
    "# tokenized_gpt = dataset.map(\n",
    "#     lambda x: tok_gpt(x['text'], truncation=True, padding='max_length'), \n",
    "#     batched=True, num_proc=None, batch_size=1_000)\n",
    "# tokenized_gpt.save_to_disk('./tokenized_dataset_small', num_proc=None)\n",
    "# # sanity check that the tokenisers are indeed identical \n",
    "# tokenized_rob = dataset.map(\n",
    "#     lambda x: tok_rob(x['text'], truncation=True, padding='max_length'), \n",
    "#     batched=True, num_proc=64, batch_size=1_000)\n",
    "\n",
    "# for split in ['train', 'validation']:\n",
    "#     for sample_gpt, sample_rob in tqdm.tqdm(zip(tokenized_gpt[split], tokenized_rob[split])):\n",
    "#         assert sample_gpt == sample_rob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk \n",
    "tokenized_dataset = load_from_disk(f'./tokenized_dataset_small') if small_dataset else load_from_disk('./tokenized_dataset')\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset  = tokenized_dataset['validation']\n",
    "\n",
    "assert len(tokenized_dataset['train'][0]['input_ids']) == config_gpt.max_position_embeddings\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:]\n",
    "# should be pad tokens (0), given that most short stories are <512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Before we get started, you may want to specify which GPU to use. See the first cell in this notebook; make sure to run it before anything else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface provides some powerful (and often confusingly long) APIs for model training. The `TrainingArguments` specifies our hyperparameters, which are used by the `Trainer` taking in the remaining objects (like `model`, `tokenizer`, and `train_dataset`). Specifically:\n",
    "\n",
    "- `learning_rate` and `num_train_epochs` determine how much the model learns. A higher rate is faster, but more unstable. More epochs (entire passes over the dataset) yields incrementally better results, at the cost of more training time. \n",
    "- Batch sizes determine how many samples the model sees in *parallel*. Given `gradient_accumulation_steps=1` and a `batch_size=8`, the model will backpropagate the average loss of 8 samples; if `batch_size=1`, it will average the loss of `gradient_accumulation_steps` samples. It is important to make sure the backpropagated loss is averaged over the same number of samples, when comparing models. \n",
    "\n",
    "- `data_collator` batches (and optionally, pads) the input for the model. We have already padded in our `tokenized_dataset`, and leaving this argument empty will automatically batch the inputs. So why do we need it? \n",
    "\n",
    "    Glad you asked. This has to do with how the loss is computed in causal language modelling. In our case, we try to predict $p(y | x)$, where $x$ is an input sequence of tokens, and $y$ is the next token following that sequence. Our model, unaware of the target token $y$, outputs $\\hat y$. \n",
    "    \n",
    "    For `Trainer` to compute the (cross-entropy) loss, we need to provide it with both $y$ and $\\hat y$. The `DataCollatorForLanguageModeling` knows this, and provides the next token $y$ as a separate part of the input, to the `Trainer`.\n",
    "\n",
    "    The loss is the backbone of backpropagation, which we need to actually improve our model. If this is confusing, please re-watch Karpathy's GPT tutorial. \n",
    "\n",
    "If you prefer to write the training loop yourself, check out HF' `run_clm_no_trainer.py` scripts. (`run_mlm_no_trainer.py` for RoBERTa-style masked-language modelling, as opposed to causal language modelling). This can be useful to give you better control over which devices are used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "def get_hyperparameters(model, dataset):\n",
    "    ''' common hyperparameters to give to the trainer '''\n",
    "\n",
    "    # TRAINING HYPERPARAMETERS \n",
    "    batch_size = 16                  # TinyStories uses 80, but I am training locally on my poor M1 Air\n",
    "    num_train_epochs = 1             # TinyStories doesn't mention\n",
    "    gradient_accumulation_steps = 16 # TinyStories uses 16\n",
    "\n",
    "    lr = 5e-4                        # TinyStories uses 5e-4, higher values better for small models\n",
    "\n",
    "    # future you will thank you for descriptive model names\n",
    "    # TODO: customise this name such that every model you train has a unique identifier!\n",
    "    config      = model.config \n",
    "    model_name  = '-'.join([\n",
    "        'KAN' if isinstance(model, CustomGPTNeoForCausalLM) else 'BERT',\n",
    "        f'{model.num_parameters()//1e6:.1f}M',\n",
    "        f'{config.num_layers if isinstance(model, GPTNeoForCausalLM) else config.num_hidden_layers}L', \n",
    "        f'{config.num_heads if isinstance(model, GPTNeoForCausalLM) else config.num_attention_heads}H', \n",
    "        f'{config.hidden_size}C',\n",
    "        f'{config.intermediate_size}I'\n",
    "    ])\n",
    "\n",
    "    _train_steps = len(dataset) // (batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = _train_steps // 10 # evaluate every 10% of training steps\n",
    "\n",
    "    return dict(\n",
    "        model_name = model_name,\n",
    "        batch_size = batch_size, \n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        lr = lr,\n",
    "        eval_steps = eval_steps\n",
    "    )\n",
    "\n",
    "params_gpt = get_hyperparameters(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(\n",
    "        model, tokenizer, train_dataset, eval_dataset, output_dir,\n",
    "        model_name, batch_size, num_train_epochs, gradient_accumulation_steps, lr, eval_steps):\n",
    "    ''' more general training arguments you likely want to keep fixed'''\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "\n",
    "        seed       = 42,\n",
    "        use_cpu    = False, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "\n",
    "        output_dir = os.path.join(output_dir, model_name),\n",
    "\n",
    "        # NOTE: training params\n",
    "        learning_rate    = lr,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        # Use a smaller batch size to fit into GPU RAM. \n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size  = batch_size,\n",
    "        # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "        # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "        # NOTE: Evaluation params\n",
    "        # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "\n",
    "        logging_first_step=True,\n",
    "        logging_steps=100,\n",
    "        report_to  = 'wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset, \n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    # print amount of training steps, and how often the model is evaluated\n",
    "    print(f'''\n",
    "    Retrieving Trainer for \\033[1m{model_name}\\033[0m\n",
    "        training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "        {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "        gives {len(train_dataset)//(batch_size * gradient_accumulation_steps)} training steps.\n",
    "        Evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "        ''')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Retrieving Trainer for KAN-16.0M-2L-4H-516C-1024I\n",
      "        training for 1 epochs, 1000 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 3 training steps.\n",
      "        Evaluating every 0 steps, 100 samples \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "out_dir = './results/models_baseline_small/' \n",
    "\n",
    "trainer_gpt = get_trainer(model, tok_gpt, train_dataset, eval_dataset, out_dir, **params_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train. \n",
    "\n",
    "This configuration takes ≤24hr to pre-train on my M1 Macbook Air with 16GB RAM. Python takes ≤4GB VRAM at a `batch_size=16` and ≤11GB at `batch_size=64`, though they take the same amount of time to train - likely because this processor is not designed to move that much data in and out of RAM constantly. And tbh, the GPU be lacking. If you decide to go the local-training-route, consider [chai](https://github.com/lvillani/chai) to keep your (Apple) laptop awake – there's probably a windows/linux equivalent too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(trainer: Trainer, name: str, out_dir: str): \n",
    "\n",
    "    wandb.init(project='tiny-transformers', name=name, group='KAN', config=trainer.args)\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(out_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words vs. tokens \n",
    "len(train_dataset['text'][11]), len(train_dataset[11]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\tiny-transformers\\code\\filip\\wandb\\run-20240511_014821-gepzalnc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/filipignijic/tiny-transformers/runs/gepzalnc' target=\"_blank\">KAN-16.0M-2L-4H-516C-1024I</a></strong> to <a href='https://wandb.ai/filipignijic/tiny-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/filipignijic/tiny-transformers' target=\"_blank\">https://wandb.ai/filipignijic/tiny-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/filipignijic/tiny-transformers/runs/gepzalnc' target=\"_blank\">https://wandb.ai/filipignijic/tiny-transformers/runs/gepzalnc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c5c52ad9514b8d80d25360bb66f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 516])\n",
      "device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 266256]' is invalid for input of size 2181169152",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_gpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_gpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36mdo_train\u001b[1;34m(trainer, name, out_dir)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_train\u001b[39m(trainer: Trainer, name: \u001b[38;5;28mstr\u001b[39m, out_dir: \u001b[38;5;28mstr\u001b[39m): \n\u001b[0;32m      3\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiny-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKAN\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msave_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, name))\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2209\u001b[0m ):\n\u001b[0;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\trainer.py:3138\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\trainer.py:3161\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3160\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3163\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:974\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    972\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 974\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:842\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    832\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    833\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    834\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    839\u001b[0m         output_attentions,\n\u001b[0;32m    840\u001b[0m     )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:584\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    582\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> 584\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[0;32m    586\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\kan\\KANLayer.py:172\u001b[0m, in \u001b[0;36mKANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mijk,l->iljk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    173\u001b[0m preacts \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)\n\u001b[0;32m    174\u001b[0m preacts \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone() \n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 266256]' is invalid for input of size 2181169152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 268, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^Exception in thread ^ChkStopThr\n",
      ":\n",
      "  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 848, in deliver_network_status\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    return self._deliver_network_status(status)    \n",
      "self.run() \n",
      "   File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "       _threading_Thread_run(self) \n",
      "   File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "        self._target(*self._args, **self._kwargs)^^\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 286, in check_stop_status\n",
      "^^^    Exception in thread ^self._loop_check_status(IntMsgThr^\n",
      ":\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "Traceback (most recent call last):\n",
      "^  File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "^    ^local_handle = request()^    \n",
      "^self.run()^ \n",
      "^ ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      " ^ ^     ^_threading_Thread_run(self) ^\n",
      " ^  File \"C:\\Users\\Filip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      " ^ ^ ^     ^self._target(*self._args, **self._kwargs) ^\n",
      " ^   File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 300, in check_internal_messages\n",
      "^ ^     ^ self._loop_check_status( ^\n",
      " ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "^^^^    ^^local_handle = request()^\n",
      "\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 510, in _deliver_network_status\n",
      " ^ ^ ^ ^ \n",
      "   File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 840, in deliver_stop_status\n",
      "        return self._deliver_stop_status(status) \n",
      "                    ^ return self._deliver_record(record)^ \n",
      "^  ^^ ^ ^^ ^ ^ ^^ ^^ ^\n",
      " ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 856, in deliver_internal_messages\n",
      " ^ ^^^    ^^return self._deliver_internal_messages(internal_message)^^\n",
      "^^ ^^ ^^ ^^ ^^ ^^^ ^^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 494, in _deliver_stop_status\n",
      "^\n",
      "  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 459, in _deliver_record\n",
      "^    ^return self._deliver_record(record)    ^\n",
      "handle = mailbox._deliver_record(record, interface=self)^ \n",
      "^ ^  ^  ^  ^  ^   ^   ^  ^  ^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 516, in _deliver_internal_messages\n",
      "^^^^^    ^^return self._deliver_record(record)^\n",
      "\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 459, in _deliver_record\n",
      " ^ ^     ^handle = mailbox._deliver_record(record, interface=self) ^\n",
      " ^  ^   ^  ^  ^  ^^ ^ ^^ ^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^\n",
      "^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "^^^^    ^^interface._publish(record)^^\n",
      "^^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "^^^^^^^^^^^^^^^\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 459, in _deliver_record\n",
      "^^^        ^handle = mailbox._deliver_record(record, interface=self)self._sock_client.send_record_publish(record)^\n",
      "\n",
      "^ ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^\n",
      "^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "^^^    ^interface._publish(record)^\n",
      "    ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "self.send_server_request(server_req)^\n",
      "^^      File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "^self._sock_client.send_record_publish(record)^\n",
      "    ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "self._send_message(msg)^\n",
      "^      File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "^self.send_server_request(server_req)^\n",
      "    ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "self._sendall_with_error_handle(header + data)^\n",
      "    ^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "self._send_message(msg)^\n",
      "^      File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "^sent = self._sock.send(data)^\n",
      "    ^ self._sendall_with_error_handle(header + data)^ ^\n",
      " ^   File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "^ ^     ^ sent = self._sock.send(data)^ ^\n",
      " ^  ^  ^^ ^^ ^^ ^^ \n",
      "^   File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "^ ^ ^     ^ interface._publish(record)^^\n",
      "^^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "^^^^^    ^^self._sock_client.send_record_publish(record)^^^\n",
      "^^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "^^^^^    self.send_server_request(server_req)^\n",
      "\n",
      "^ConnectionResetError^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      ": ^[WinError 10054] An existing connection was forcibly closed by the remote host^\n",
      "^    ^self._send_message(msg)^^\n",
      "^  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "^\n",
      "    ConnectionResetErrorself._sendall_with_error_handle(header + data): \n",
      "[WinError 10054] An existing connection was forcibly closed by the remote host  File \"c:\\Users\\Filip\\Desktop\\Schoo_Projects\\Y3\\Research\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "\n",
      "    sent = self._sock.send(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    }
   ],
   "source": [
    "do_train(trainer_gpt, params_gpt['model_name'], out_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your Pre-Trained Model \n",
    "Try out your own model on some prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I made a small function for pretty printing into paragraphs with line wrapping for readability\n",
    "#import textwrap \n",
    "#w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "#def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "#                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#\n",
    "#model_name = 'GPT-9.0M-2L-4H-516C-1024I'\n",
    "## model_name = 'BERT-9.0M-2L-4H-516C-1024I' # bert won't work for generation unless you fine-tune it for that task\n",
    "#\n",
    "#tokenizer = AutoTokenizer.from_pretrained('10k-tok')\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#model = AutoModelForCausalLM.from_pretrained(f'results/models_baseline_small/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference \n",
    "Let's generate a short story like the ones the model has been trained on! You'll notice that the prompt is surrounded by `<s>`, the begin-of-sequence (bos) token, and `</s>` end-of-sequence (eos) / separator (sep) token. This is from the BERT-style tokenisation, making it clear to the model where (one of several) input sequences ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   <s>Once upon a time, there was a chicken\n",
      "  with</s>................................................................................................................................................................................................................................................................................................\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 7454, 2402,  257,  640,   11,  612,  373,  257, 9015,  351,    2,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prompt = 'Once upon a time, there was a chicken with'\n",
    "#input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#\n",
    "#output = model.generate(\n",
    "#    input_ids,                              # input to the model\n",
    "#    max_length=300,                         # maximum generation length\n",
    "#    eos_token_id=tokenizer.eos_token_id,    # early stopping when eos_token is output\n",
    "#\n",
    "#    # num_beams=1,                            # number of beams to use in generation\n",
    "#    temperature=1,\n",
    "#)\n",
    "#output_text = tokenizer.decode(output[0])\n",
    "#\n",
    "## textwrap with indentation on every new paragraph\n",
    "#see(output_text)\n",
    "#output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds like me after a few beers too many, but at least the grammar is (mostly) correct. The model also learns some basic reasoning-like associations like being 'so high' allows you to see 'the whole world'. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
