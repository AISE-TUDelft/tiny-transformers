{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training [`GPT-Neo`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py) & [`RoBERTa`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py) on [`TinyStories`](https://huggingface.co/datasets/roneneldan/TinyStories)\n",
    "\n",
    "This script shows how to pre-train both models. I put them in one notebook because the majority of the code is shared; but you may want to separate the logic per model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch wandb transformers[torch] datasets tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "from typing import Optional, Tuple\n",
    "import wandb; wandb.login()\n",
    "import torch\n",
    "from transformers import (\n",
    "    RobertaForMaskedLM, RobertaConfig, RobertaTokenizerFast,\n",
    "    GPTNeoForCausalLM, GPTNeoConfig, GPT2TokenizerFast\n",
    ")\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models \n",
    "We consider GPT-Neo and BERT as base transformer architectures. This consists of the following blocks linked via residual connections:\n",
    "\n",
    "- Embeddings: Map one-hot (sparse) token vectors to a dense vector of length `hidden_size`. Add positional encoding. \n",
    "- $n$ Blocks: Contain self-attention and FFNs.\n",
    "- Head: Map hidden state back to a useful output. \n",
    "\n",
    "This specifies some of the model-related hyperparameters. I chose them based on what achieved reasonable performance in the [TinyStories paper](https://arxiv.org/abs/2305.07759), while also being feasible to train on our limited compute budgets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gpt = dict(\n",
    "\n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   # number of tokens in the vocabulary \n",
    "    hidden_size             = 516,      # embedding size (vector length) of each token \n",
    "    max_position_embeddings = 512,      # maximum sequence length (context window)\n",
    "\n",
    "    # BLOCKS (ATTN & FFN)\n",
    "    num_layers          = 2,                    # number of transformer blocks\n",
    "    attention_types     = [[[\"global\", \"local\"], 1]], # (GPT-Neo-specific) global and local attention \n",
    "    num_heads           = 4,                    # attention heads\n",
    "    window_size         = 256,                  # (GPT-Neo-specific) for local attention \n",
    "    intermediate_size   = 1024,                 # size of 'up-projection' layer in FFN\n",
    "\n",
    "    pad_token_id = 0,           # need to specify this for tokenizer interop between models\n",
    ")\n",
    "\n",
    "config_rob = dict(\n",
    "    \n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   \n",
    "    hidden_size             = 516,      \n",
    "    # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "    max_position_embeddings = config_gpt['max_position_embeddings'] + 1,\n",
    "\n",
    "    # BLOCKS (of course naming is different in roberta :) )\n",
    "    num_hidden_layers = config_gpt['num_layers'],\n",
    "    num_attention_heads = config_gpt['num_heads'],\n",
    "    intermediate_size=1024,                     \n",
    "\n",
    "    pad_token_id = 0,\n",
    ")\n",
    "\n",
    "config_gpt = GPTNeoConfig(**config_gpt)\n",
    "config_rob = RobertaConfig(**config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from https://github.com/jlamprou/Infini-Attention/blob/main/infiniAttention.py\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTNeoSelfAttention2(nn.Module):\n",
    "    def __init__(self, config, attention_type):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        bias = torch.tril(torch.ones((max_positions, max_positions), dtype=bool)).view(\n",
    "            1, 1, max_positions, max_positions\n",
    "        )\n",
    "\n",
    "        # local causal self attention is a sliding window where each token can only attend to the previous\n",
    "        # window_size tokens. This is implemented by updating the causal mask such that for each token\n",
    "        # all other tokens are masked except the previous window_size tokens.\n",
    "        if attention_type == \"local\":\n",
    "            bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n",
    "\n",
    "        self.register_buffer(\"bias\", bias, persistent=False)\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e9), persistent=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(float(config.attention_dropout))\n",
    "        self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        \n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "        self.ELU = nn.ELU()\n",
    "        self.register_buffer(\"M\", torch.zeros(self.num_heads, self.head_dim, self.head_dim))\n",
    "        self.register_buffer(\"z\", torch.zeros(self.num_heads, self.head_dim))\n",
    "\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def _attn(self, query, key, value, compressive_memory, z, attention_mask=None, head_mask=None):\n",
    "        # Keep the attention weights computation in fp32 to avoid overflow issues\n",
    "        query = query.to(torch.float32)\n",
    "        key = key.to(torch.float32)\n",
    "\n",
    "        sigma_q = self.ELU(query[:, :, :, :]) + 1.0\n",
    "        sigma_k = self.ELU(key[:, :, :, :]) + 1.0\n",
    "\n",
    "        A_mem = ((sigma_q @ compressive_memory) / ((sigma_q @ z) + 1e-6))\n",
    "        A_dot = query[:, :, :, :] @ key[:, :, :, :].transpose(-2, -1)\n",
    "\n",
    "        query_length, key_length = query.size(-2), key.size(-2)\n",
    "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        mask_value = torch.finfo(A_dot.dtype).min\n",
    "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "        mask_value = torch.tensor(mask_value, dtype=A_dot.dtype).to(A_dot.device)\n",
    "        A_dot = torch.where(causal_mask, A_dot, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            A_dot = A_dot + attention_mask\n",
    "\n",
    "        # attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        A_dot = A_dot.to(value.dtype)\n",
    "        A_dot = self.attn_dropout(A_dot)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            A_dot = A_dot * head_mask\n",
    "            \n",
    "        A_dot = nn.functional.softmax(A_dot / torch.sqrt(torch.tensor(self.head_dim)), dim = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        A_dot = torch.matmul(A_dot, value)\n",
    "        attention_output = (F.sigmoid(self.beta) * A_mem) + ((1 - F.sigmoid(self.beta)) * A_dot)\n",
    "\n",
    "        delta = (sigma_k @ compressive_memory) / ((sigma_k @ z) + 1e-6)\n",
    "        compressive_memory = compressive_memory + (sigma_k.transpose(-2, -1) @ (value[:, :, :, :] - delta))\n",
    "\n",
    "        z = z + sigma_k.sum(dim = -2, keepdim = True)\n",
    "\n",
    "        return attention_output, A_dot\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "        device = query.device\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        compressive_memory = torch.zeros((self.num_heads, self.head_dim, self.head_dim), device=device)\n",
    "        z = torch.zeros((self.num_heads, self.head_dim, 1), device=device)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key = layer_past[0]\n",
    "            past_value = layer_past[1]\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        attn_output, attn_weights = self._attn(query, key, value, compressive_memory, z, attention_mask, head_mask)\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,677,050 parameters.\n"
     ]
    }
   ],
   "source": [
    "class CustomGPTNeoForCausalLM(GPTNeoForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Assuming attention types are specified in config.attention_layers\n",
    "        # Replace the standard attention with the specified type\n",
    "        for block in self.transformer.h:\n",
    "            block.attn.attention = GPTNeoSelfAttention2(config_gpt, block.attn.attention_type)\n",
    "model = CustomGPTNeoForCausalLM( \n",
    "    config = config_gpt,\n",
    ")\n",
    "print(f'The model has {model.num_parameters():,} parameters.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This GPT has 9,677,050 parameters,\n",
      "     and ROB has 9,959,496 parameters.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: you should set all pytorch & huggingface seeds here as model initialisation depends on it\n",
    "\n",
    "gpt = CustomGPTNeoForCausalLM(config=config_gpt)\n",
    "rob = RobertaForMaskedLM(config=config_rob)\n",
    "\n",
    "print(f'''\n",
    "    This GPT has {gpt.num_parameters():,} parameters,\n",
    "     and ROB has {rob.num_parameters():,} parameters.\n",
    "    ''')\n",
    "\n",
    "# gpt, rob # uncomment to see model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n",
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "def get_tokenizer_for_config(Tok: PreTrainedTokenizer, config: PretrainedConfig):\n",
    "\n",
    "    tokenizer = Tok.from_pretrained(\n",
    "        '10k-tok',                                         # our custom tokenizer\n",
    "        model_max_length=config.max_position_embeddings    # sequence length (context window)\n",
    "    )\n",
    "\n",
    "    # we're using our special tokenizer with only 10'000 tokens instead of 50'256\n",
    "    assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "    print(f'padding token is {tokenizer.pad_token}')\n",
    "    print(f'padding token in config: {config.pad_token_id}, in tokeniser: {tokenizer.pad_token_id}')\n",
    "    \n",
    "    return tokenizer \n",
    "\n",
    "tok_gpt = get_tokenizer_for_config(GPT2TokenizerFast, config_gpt)\n",
    "tok_rob = get_tokenizer_for_config(RobertaTokenizerFast, config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk \n",
    "\n",
    "if debug_mode:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset_small')\n",
    "else:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset')\n",
    "\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset  = tokenized_dataset['validation']\n",
    "\n",
    "assert len(tokenized_dataset['train'][0]['input_ids']) == config_gpt.max_position_embeddings\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:]\n",
    "# should be pad tokens (0), given that most short stories are <512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Before we get started, you may want to specify which GPU to use. See the first cell in this notebook; make sure to run it before anything else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface provides some powerful (and often confusingly long) APIs for model training. The `TrainingArguments` specifies our hyperparameters, which are used by the `Trainer` taking in the remaining objects (like `model`, `tokenizer`, and `train_dataset`). Specifically:\n",
    "\n",
    "- `learning_rate` and `num_train_epochs` determine how much the model learns. A higher rate is faster, but more unstable. More epochs (entire passes over the dataset) yields incrementally better results, at the cost of more training time. \n",
    "- Batch sizes determine how many samples the model sees in *parallel*. Given `gradient_accumulation_steps=1` and a `batch_size=8`, the model will backpropagate the average loss of 8 samples; if `batch_size=1`, it will average the loss of `gradient_accumulation_steps` samples. It is important to make sure the backpropagated loss is averaged over the same number of samples, when comparing models. \n",
    "\n",
    "- `data_collator` batches (and optionally, pads) the input for the model. We have already padded in our `tokenized_dataset`, and leaving this argument empty will automatically batch the inputs. So why do we need it? \n",
    "\n",
    "    Glad you asked. This has to do with how the loss is computed in causal language modelling. In our case, we try to predict $p(y | x)$, where $x$ is an input sequence of tokens, and $y$ is the next token following that sequence. Our model, unaware of the target token $y$, outputs $\\hat y$. \n",
    "    \n",
    "    For `Trainer` to compute the (cross-entropy) loss, we need to provide it with both $y$ and $\\hat y$. The `DataCollatorForLanguageModeling` knows this, and provides the next token $y$ as a separate part of the input, to the `Trainer`.\n",
    "\n",
    "    The loss is the backbone of backpropagation, which we need to actually improve our model. If this is confusing, please re-watch Karpathy's GPT tutorial. \n",
    "\n",
    "If you prefer to write the training loop yourself, check out HF' `run_clm_no_trainer.py` scripts. (`run_mlm_no_trainer.py` for RoBERTa-style masked-language modelling, as opposed to causal language modelling). This can be useful to give you better control over which devices are used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "def get_hyperparameters(model, dataset):\n",
    "    ''' common hyperparameters to give to the trainer '''\n",
    "\n",
    "    # TRAINING HYPERPARAMETERS \n",
    "    batch_size = 16                  # TinyStories uses 80, but I am training locally on my poor M1 Air\n",
    "    num_train_epochs = 1             # TinyStories doesn't mention\n",
    "    gradient_accumulation_steps = 16 # TinyStories uses 16\n",
    "\n",
    "    lr = 5e-4                        # TinyStories uses 5e-4, higher values better for small models\n",
    "\n",
    "    # future you will thank you for descriptive model names\n",
    "    # TODO: customise this name such that every model you train has a unique identifier!\n",
    "    config      = model.config \n",
    "    model_name  = '-'.join([\n",
    "        'GPT' if isinstance(model, GPTNeoForCausalLM) else 'BERT',\n",
    "        f'{model.num_parameters()//1e6:.1f}M',\n",
    "        f'{config.num_layers if isinstance(model, GPTNeoForCausalLM) else config.num_hidden_layers}L', \n",
    "        f'{config.num_heads if isinstance(model, GPTNeoForCausalLM) else config.num_attention_heads}H', \n",
    "        f'{config.hidden_size}C',\n",
    "        f'{config.intermediate_size}I'\n",
    "    ])\n",
    "\n",
    "    _train_steps = len(dataset) // (batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = _train_steps // 10 # evaluate every 10% of training steps\n",
    "\n",
    "    return dict(\n",
    "        model_name = model_name,\n",
    "        batch_size = batch_size, \n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        lr = lr,\n",
    "        eval_steps = eval_steps\n",
    "    )\n",
    "\n",
    "params_gpt = get_hyperparameters(gpt, train_dataset)\n",
    "params_rob = get_hyperparameters(rob, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(\n",
    "        model, tokenizer, train_dataset, eval_dataset, output_dir,\n",
    "        model_name, batch_size, num_train_epochs, gradient_accumulation_steps, lr, eval_steps):\n",
    "    ''' more general training arguments you likely want to keep fixed'''\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "\n",
    "        seed       = 42,\n",
    "        #use_cpu    = False, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "\n",
    "        output_dir = os.path.join(output_dir, model_name),\n",
    "\n",
    "        # NOTE: training params\n",
    "        learning_rate    = lr,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        # Use a smaller batch size to fit into GPU RAM. \n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size  = batch_size,\n",
    "        # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "        # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "        # NOTE: Evaluation params\n",
    "        # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "\n",
    "        logging_first_step=True,\n",
    "        logging_steps=100,\n",
    "        report_to  = 'wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset, \n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=isinstance(model, RobertaForMaskedLM)),\n",
    "    )\n",
    "\n",
    "    # print amount of training steps, and how often the model is evaluated\n",
    "    print(f'''\n",
    "    Retrieving Trainer for \\033[1m{model_name}\\033[0m\n",
    "        training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "        {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "        gives {len(train_dataset)//(batch_size * gradient_accumulation_steps)} training steps.\n",
    "        Evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "        ''')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Retrieving Trainer for \u001b[1mGPT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 1000 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 3 training steps.\n",
      "        Evaluating every 0 steps, 100 samples \n",
      "        \n",
      "\n",
      "    Retrieving Trainer for \u001b[1mBERT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 1000 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 3 training steps.\n",
      "        Evaluating every 0 steps, 100 samples \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "out_dir = './results/models_baseline/' \n",
    "\n",
    "trainer_gpt = get_trainer(gpt, tok_gpt, train_dataset, eval_dataset, out_dir, **params_gpt)\n",
    "trainer_rob = get_trainer(rob, tok_rob, train_dataset, eval_dataset, out_dir, **params_rob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train. \n",
    "\n",
    "This configuration takes ≤24hr to pre-train on my M1 Macbook Air with 16GB RAM. Python takes ≤4GB VRAM at a `batch_size=16` and ≤11GB at `batch_size=64`, though they take the same amount of time to train - likely because this processor is not designed to move that much data in and out of RAM constantly. And tbh, the GPU be lacking. If you decide to go the local-training-route, consider [chai](https://github.com/lvillani/chai) to keep your (Apple) laptop awake – there's probably a windows/linux equivalent too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(trainer: Trainer, name: str, out_dir: str): \n",
    "\n",
    "    wandb.init(project='tiny-transformers', name=name, group='baseline', config=trainer.args)\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(out_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 512)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words vs. tokens \n",
    "len(train_dataset['text'][11]), len(train_dataset[11]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_train(trainer_rob, params_rob['model_name'], out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_train(trainer_gpt, params_gpt['model_name'], out_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your Pre-Trained Model \n",
    "Try out your own model on some prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a small function for pretty printing into paragraphs with line wrapping for readability\n",
    "import textwrap \n",
    "w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomGPTNeoForCausalLM were not initialized from the model checkpoint at results/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I and are newly initialized: ['transformer.h.0.attn.attention.M', 'transformer.h.0.attn.attention.beta', 'transformer.h.0.attn.attention.z', 'transformer.h.1.attn.attention.M', 'transformer.h.1.attn.attention.beta', 'transformer.h.1.attn.attention.z']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomGPTNeoForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = 'GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I'\n",
    "vanilla = 'GPT-9.0M-2L-4H-516C-1024I'\n",
    "# model_name = 'BERT-9.0M-2L-4H-516C-1024I' # bert won't work for generation unless you fine-tune it for that task\n",
    "infin_name = 'GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I'\n",
    "tokenizer = AutoTokenizer.from_pretrained('10k-tok')\n",
    "tokenizer_old = AutoTokenizer.from_pretrained('10k-gpt-neo')\n",
    "\n",
    "tokenizer_old.pad_token_id = tokenizer.eos_token_id\n",
    "model = CustomGPTNeoForCausalLM.from_pretrained(f'results/models_baseline/{infin_name}')\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference \n",
    "Let's generate a short story like the ones the model has been trained on! You'll notice that the prompt is surrounded by `<s>`, the begin-of-sequence (bos) token, and `</s>` end-of-sequence (eos) / separator (sep) token. This is from the BERT-style tokenisation, making it clear to the model where (one of several) input sequences ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m   Once upon a time, there was a little car named Beep.\n",
      "\n",
      "   One day, Beep was playing in the park. Suddenly, a big,\n",
      "  scary monster appeared. The monster was very big and\n",
      "  scary. Beep was very scared.\n",
      "\n",
      "   \"Hello, Beep!\" said Beep. \"What are you doing?\"\n",
      "\n",
      "   The monster replied, \"I am just playing. I am scared of\n",
      "  the big, scary monster.\"\n",
      "\n",
      "   The monster said, \"Don't be scared. I will protect you.\"\n",
      "\n",
      "   The monster said, \"I will protect you. I will protect\n",
      "  you.\"\n",
      "\n",
      "   The monster said, \"Thank you, Beep. You are very kind.\"\n",
      "\n",
      "   The monster said, \"You're welcome. I'm glad you are\n",
      "  safe.\"\n",
      "\n",
      "   The monster said, \"You're welcome. I'm glad you are\n",
      "  safe.\"\n",
      "\n",
      "   And they lived happily ever\n",
      "  after.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257, 1310, 1097, 3706, 1355,\n",
       "          538,   13,  220,  198,  198, 3198, 1110,   11, 1355,  538,  373, 2712,\n",
       "          287,  262, 3952,   13,  311, 4185,  268,  306,   11,  257, 1263,   11,\n",
       "          629,  560, 9234, 4120,   13,  383, 9234,  373,  845, 1263,  290,  629,\n",
       "          560,   13, 1355,  538,  373,  845,  629, 1144,   13,  198,  198,    1,\n",
       "           39,  695,   78,   11, 1355,  538, 2474,  531, 1355,  538,   13,  366,\n",
       "         2061,  389,  345, 1804, 1701,  198,  198,  464, 9234, 8712,   11,  366,\n",
       "           40,  716,  655, 2712,   13,  314,  716,  629, 1144,  286,  262, 1263,\n",
       "           11,  629,  560, 9234,  526,  198,  198,  464, 9234,  531,   11,  366,\n",
       "         3987,  470,  307,  629, 1144,   13,  314,  481, 1805,  345,  526,  198,\n",
       "          198,  464, 9234,  531,   11,  366,   40,  481, 1805,  345,   13,  314,\n",
       "          481, 1805,  345,  526,  198,  198,  464, 9234,  531,   11,  366,  817,\n",
       "          962,  345,   11, 1355,  538,   13,  921,  389,  845, 1611,  526,  198,\n",
       "          198,  464, 9234,  531,   11,  366, 1639,  821, 7062,   13,  314, 1101,\n",
       "         9675,  345,  389, 3338,  526,  198,  198,  464, 9234,  531,   11,  366,\n",
       "         1639,  821, 7062,   13,  314, 1101, 9675,  345,  389, 3338,  526,  198,\n",
       "          198, 1870,  484, 5615, 1147,  813, 1683,  706,   13, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "         9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Once upon a time, there was a little car named Beep. '\n",
    "input_ids = tokenizer_old.encode(prompt, return_tensors='pt')\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,                              # input to the model\n",
    "    max_length=300,                         # maximum generation length\n",
    "    eos_token_id=tokenizer.eos_token_id,    # early stopping when eos_token is output\n",
    "\n",
    "    # num_beams=1,                            # number of beams to use in generation\n",
    "    temperature=1,\n",
    ")\n",
    "# print(output)\n",
    "output_text = tokenizer_old.decode(output[0])\n",
    "\n",
    "# textwrap with indentation on every new paragraph\n",
    "see(output_text)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a dict of [str, torch.Tensor] but received <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb Cell 29\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mresults/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I/checkpoint-8280/pytorch_model.bin\u001b[39m\u001b[39m'\u001b[39m, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y111sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Save the model parameters in SafeTensor format directly since it's already a dictionary\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y111sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m save(\u001b[39m'\u001b[39;49m\u001b[39mresults/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I/checkpoint-8280/model.safetensors\u001b[39;49m\u001b[39m'\u001b[39;49m, model_dict)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/safetensors/torch.py:248\u001b[0m, in \u001b[0;36msave\u001b[0;34m(tensors, metadata)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor], metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[1;32m    224\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     serialized \u001b[39m=\u001b[39m serialize(_flatten(tensors), metadata\u001b[39m=\u001b[39mmetadata)\n\u001b[1;32m    249\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(serialized)\n\u001b[1;32m    250\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/safetensors/torch.py:457\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten\u001b[39m(tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 457\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict of [str, torch.Tensor] but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m     invalid_tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m    460\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a dict of [str, torch.Tensor] but received <class 'str'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import save\n",
    "\n",
    "# Load the PyTorch model\n",
    "model_dict = torch.load('results/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I/checkpoint-8280/pytorch_model.bin', map_location=torch.device('cpu'))\n",
    "\n",
    "# Save the model parameters in SafeTensor format directly since it's already a dictionary\n",
    "save('results/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I/checkpoint-8280/model.safetensors', model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds like me after a few beers too many, but at least the grammar is (mostly) correct. The model also learns some basic reasoning-like associations like being 'so high' allows you to see 'the whole world'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--revision REVISION] [--force] [-y]\n",
      "                             model_id\n",
      "ipykernel_launcher.py: error: argument --force: ignored explicit argument '/Users/laurikeskull/Library/Jupyter/runtime/kernel-v2-19756pxLDKO6ggPAK.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:1902\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     namespace, args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_known_args(args, namespace)\n\u001b[1;32m   1903\u001b[0m \u001b[39mexcept\u001b[39;00m ArgumentError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:2114\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[39m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2114\u001b[0m     start_index \u001b[39m=\u001b[39m consume_optional(start_index)\n\u001b[1;32m   2116\u001b[0m \u001b[39m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:2036\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2035\u001b[0m         msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39mignored explicit argument \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2036\u001b[0m         \u001b[39mraise\u001b[39;00m ArgumentError(action, msg \u001b[39m%\u001b[39m explicit_arg)\n\u001b[1;32m   2038\u001b[0m \u001b[39m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m \u001b[39m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m \u001b[39m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --force: ignored explicit argument '/Users/laurikeskull/Library/Jupyter/runtime/kernel-v2-19756pxLDKO6ggPAK.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=351'>352</a>\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=352'>353</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m-y\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=353'>354</a>\u001b[0m     action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstore_true\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=354'>355</a>\u001b[0m     help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIgnore safety prompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=355'>356</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=356'>357</a>\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mparse_args()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y113sZmlsZQ%3D%3D?line=357'>358</a>\u001b[0m model_id \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mmodel_id\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:1869\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_args\u001b[39m(\u001b[39mself\u001b[39m, args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, namespace\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1869\u001b[0m     args, argv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_known_args(args, namespace)\n\u001b[1;32m   1870\u001b[0m     \u001b[39mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:1904\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[39mexcept\u001b[39;00m ArgumentError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 1904\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(\u001b[39mstr\u001b[39;49m(err))\n\u001b[1;32m   1905\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2629\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[0;32m-> 2630\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m, _(\u001b[39m'\u001b[39;49m\u001b[39m%(prog)s\u001b[39;49;00m\u001b[39m: error: \u001b[39;49m\u001b[39m%(message)s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m%\u001b[39;49m args)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(message, _sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 2617\u001b[0m _sys\u001b[39m.\u001b[39;49mexit(status)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/wandb/sdk/lib/exit_hooks.py:36\u001b[0m, in \u001b[0;36mExitHooks.exit\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit_code \u001b[39m=\u001b[39m code\n\u001b[0;32m---> 36\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_exit(orig_code)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[1;32m   2146\u001b[0m                                                      value))\n\u001b[1;32m   2147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m    569\u001b[0m             etype,\n\u001b[1;32m    570\u001b[0m             evalue,\n\u001b[1;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[1;32m    573\u001b[0m             context,\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1455\u001b[0m     \u001b[39mself\u001b[39;49m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1456\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[1;32m   1346\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[1;32m   1348\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m   1193\u001b[0m                                                            tb_offset)\n\u001b[1;32m   1195\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(\u001b[39mstr\u001b[39m(etype), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[39mwhile\u001b[39;00m cf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetmodule(cf\u001b[39m.\u001b[39;49mtb_frame)\n\u001b[1;32m   1151\u001b[0m         \u001b[39mif\u001b[39;00m mod \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import CommitInfo, CommitOperationAdd, Discussion, HfApi, hf_hub_download\n",
    "from huggingface_hub.file_download import repo_folder_name\n",
    "from safetensors.torch import _find_shared_tensors, _is_complete, load_file, save_file\n",
    "\n",
    "\n",
    "COMMIT_DESCRIPTION = \"\"\"\n",
    "This is an automated PR created with https://huggingface.co/spaces/safetensors/convert\n",
    "\n",
    "This new file is equivalent to `pytorch_model.bin` but safe in the sense that\n",
    "no arbitrary code can be put into it.\n",
    "\n",
    "These files also happen to load much faster than their pytorch counterpart:\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb\n",
    "\n",
    "The widgets on your model page will run using this model even if this is not merged\n",
    "making sure the file actually works.\n",
    "\n",
    "If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions\n",
    "\n",
    "Feel free to ignore this PR.\n",
    "\"\"\"\n",
    "\n",
    "ConversionResult = Tuple[List[\"CommitOperationAdd\"], List[Tuple[str, \"Exception\"]]]\n",
    "\n",
    "\n",
    "def _remove_duplicate_names(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    *,\n",
    "    preferred_names: List[str] = None,\n",
    "    discard_names: List[str] = None,\n",
    ") -> Dict[str, List[str]]:\n",
    "    if preferred_names is None:\n",
    "        preferred_names = []\n",
    "    preferred_names = set(preferred_names)\n",
    "    if discard_names is None:\n",
    "        discard_names = []\n",
    "    discard_names = set(discard_names)\n",
    "\n",
    "    shareds = _find_shared_tensors(state_dict)\n",
    "    to_remove = defaultdict(list)\n",
    "    for shared in shareds:\n",
    "        complete_names = set([name for name in shared if _is_complete(state_dict[name])])\n",
    "        if not complete_names:\n",
    "            if len(shared) == 1:\n",
    "                # Force contiguous\n",
    "                name = list(shared)[0]\n",
    "                state_dict[name] = state_dict[name].clone()\n",
    "                complete_names = {name}\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"Error while trying to find names to remove to save state dict, but found no suitable name to keep for saving amongst: {shared}. None is covering the entire storage.Refusing to save/load the model since you could be storing much more memory than needed. Please refer to https://huggingface.co/docs/safetensors/torch_shared_tensors for more information. Or open an issue.\"\n",
    "                )\n",
    "\n",
    "        keep_name = sorted(list(complete_names))[0]\n",
    "\n",
    "        # Mecanism to preferentially select keys to keep\n",
    "        # coming from the on-disk file to allow\n",
    "        # loading models saved with a different choice\n",
    "        # of keep_name\n",
    "        preferred = complete_names.difference(discard_names)\n",
    "        if preferred:\n",
    "            keep_name = sorted(list(preferred))[0]\n",
    "\n",
    "        if preferred_names:\n",
    "            preferred = preferred_names.intersection(complete_names)\n",
    "            if preferred:\n",
    "                keep_name = sorted(list(preferred))[0]\n",
    "        for name in sorted(shared):\n",
    "            if name != keep_name:\n",
    "                to_remove[keep_name].append(name)\n",
    "    return to_remove\n",
    "\n",
    "\n",
    "def get_discard_names(model_id: str, revision: Optional[str], folder: str, token: Optional[str]) -> List[str]:\n",
    "    try:\n",
    "        import json\n",
    "\n",
    "        import transformers\n",
    "\n",
    "        config_filename = hf_hub_download(\n",
    "            model_id, revision=revision, filename=\"config.json\", token=token, cache_dir=folder\n",
    "        )\n",
    "        with open(config_filename, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        architecture = config[\"architectures\"][0]\n",
    "\n",
    "        class_ = getattr(transformers, architecture)\n",
    "\n",
    "        # Name for this varible depends on transformers version.\n",
    "        discard_names = getattr(class_, \"_tied_weights_keys\", [])\n",
    "\n",
    "    except Exception:\n",
    "        discard_names = []\n",
    "    return discard_names\n",
    "\n",
    "\n",
    "class AlreadyExists(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_file_size(sf_filename: str, pt_filename: str):\n",
    "    sf_size = os.stat(sf_filename).st_size\n",
    "    pt_size = os.stat(pt_filename).st_size\n",
    "\n",
    "    if (sf_size - pt_size) / pt_size > 0.01:\n",
    "        raise RuntimeError(\n",
    "            f\"\"\"The file size different is more than 1%:\n",
    "         - {sf_filename}: {sf_size}\n",
    "         - {pt_filename}: {pt_size}\n",
    "         \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def rename(pt_filename: str) -> str:\n",
    "    filename, ext = os.path.splitext(pt_filename)\n",
    "    local = f\"{filename}.safetensors\"\n",
    "    local = local.replace(\"pytorch_model\", \"model\")\n",
    "    return local\n",
    "\n",
    "\n",
    "def convert_multi(\n",
    "    model_id: str, *, revision=Optional[str], folder: str, token: Optional[str], discard_names: List[str]\n",
    ") -> ConversionResult:\n",
    "    filename = hf_hub_download(\n",
    "        repo_id=model_id, revision=revision, filename=\"pytorch_model.bin.index.json\", token=token, cache_dir=folder\n",
    "    )\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filenames = set(data[\"weight_map\"].values())\n",
    "    local_filenames = []\n",
    "    for filename in filenames:\n",
    "        pt_filename = hf_hub_download(repo_id=model_id, filename=filename, token=token, cache_dir=folder)\n",
    "\n",
    "        sf_filename = rename(pt_filename)\n",
    "        sf_filename = os.path.join(folder, sf_filename)\n",
    "        convert_file(pt_filename, sf_filename, discard_names=discard_names)\n",
    "        local_filenames.append(sf_filename)\n",
    "\n",
    "    index = os.path.join(folder, \"model.safetensors.index.json\")\n",
    "    with open(index, \"w\") as f:\n",
    "        newdata = {k: v for k, v in data.items()}\n",
    "        newmap = {k: rename(v) for k, v in data[\"weight_map\"].items()}\n",
    "        newdata[\"weight_map\"] = newmap\n",
    "        json.dump(newdata, f, indent=4)\n",
    "    local_filenames.append(index)\n",
    "\n",
    "    operations = [\n",
    "        CommitOperationAdd(path_in_repo=os.path.basename(local), path_or_fileobj=local) for local in local_filenames\n",
    "    ]\n",
    "    errors: List[Tuple[str, \"Exception\"]] = []\n",
    "\n",
    "    return operations, errors\n",
    "\n",
    "\n",
    "def convert_single(\n",
    "    model_id: str, *, revision: Optional[str], folder: str, token: Optional[str], discard_names: List[str]\n",
    ") -> ConversionResult:\n",
    "    pt_filename = hf_hub_download(\n",
    "        repo_id=model_id, revision=revision, filename=\"pytorch_model.bin\", token=token, cache_dir=folder\n",
    "    )\n",
    "\n",
    "    sf_name = \"model.safetensors\"\n",
    "    sf_filename = os.path.join(folder, sf_name)\n",
    "    convert_file(pt_filename, sf_filename, discard_names)\n",
    "    operations = [CommitOperationAdd(path_in_repo=sf_name, path_or_fileobj=sf_filename)]\n",
    "    errors: List[Tuple[str, \"Exception\"]] = []\n",
    "    return operations, errors\n",
    "\n",
    "\n",
    "def convert_file(\n",
    "    pt_filename: str,\n",
    "    sf_filename: str,\n",
    "    discard_names: List[str],\n",
    "):\n",
    "    loaded = torch.load(pt_filename, map_location=\"cpu\")\n",
    "    if \"state_dict\" in loaded:\n",
    "        loaded = loaded[\"state_dict\"]\n",
    "    to_removes = _remove_duplicate_names(loaded, discard_names=discard_names)\n",
    "\n",
    "    metadata = {\"format\": \"pt\"}\n",
    "    for kept_name, to_remove_group in to_removes.items():\n",
    "        for to_remove in to_remove_group:\n",
    "            if to_remove not in metadata:\n",
    "                metadata[to_remove] = kept_name\n",
    "            del loaded[to_remove]\n",
    "    # Force tensors to be contiguous\n",
    "    loaded = {k: v.contiguous() for k, v in loaded.items()}\n",
    "\n",
    "    dirname = os.path.dirname(sf_filename)\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    save_file(loaded, sf_filename, metadata=metadata)\n",
    "    check_file_size(sf_filename, pt_filename)\n",
    "    reloaded = load_file(sf_filename)\n",
    "    for k in loaded:\n",
    "        pt_tensor = loaded[k]\n",
    "        sf_tensor = reloaded[k]\n",
    "        if not torch.equal(pt_tensor, sf_tensor):\n",
    "            raise RuntimeError(f\"The output tensors do not match for key {k}\")\n",
    "\n",
    "\n",
    "def create_diff(pt_infos: Dict[str, List[str]], sf_infos: Dict[str, List[str]]) -> str:\n",
    "    errors = []\n",
    "    for key in [\"missing_keys\", \"mismatched_keys\", \"unexpected_keys\"]:\n",
    "        pt_set = set(pt_infos[key])\n",
    "        sf_set = set(sf_infos[key])\n",
    "\n",
    "        pt_only = pt_set - sf_set\n",
    "        sf_only = sf_set - pt_set\n",
    "\n",
    "        if pt_only:\n",
    "            errors.append(f\"{key} : PT warnings contain {pt_only} which are not present in SF warnings\")\n",
    "        if sf_only:\n",
    "            errors.append(f\"{key} : SF warnings contain {sf_only} which are not present in PT warnings\")\n",
    "    return \"\\n\".join(errors)\n",
    "\n",
    "\n",
    "def previous_pr(api: \"HfApi\", model_id: str, pr_title: str, revision=Optional[str]) -> Optional[\"Discussion\"]:\n",
    "    try:\n",
    "        revision_commit = api.model_info(model_id, revision=revision).sha\n",
    "        discussions = api.get_repo_discussions(repo_id=model_id)\n",
    "    except Exception:\n",
    "        return None\n",
    "    for discussion in discussions:\n",
    "        if discussion.status in {\"open\", \"closed\"} and discussion.is_pull_request and discussion.title == pr_title:\n",
    "            commits = api.list_repo_commits(model_id, revision=discussion.git_reference)\n",
    "\n",
    "            if revision_commit == commits[1].commit_id:\n",
    "                return discussion\n",
    "    return None\n",
    "\n",
    "\n",
    "def convert_generic(\n",
    "    model_id: str, *, revision=Optional[str], folder: str, filenames: Set[str], token: Optional[str]\n",
    ") -> ConversionResult:\n",
    "    operations = []\n",
    "    errors = []\n",
    "\n",
    "    extensions = set([\".bin\", \".ckpt\"])\n",
    "    for filename in filenames:\n",
    "        prefix, ext = os.path.splitext(filename)\n",
    "        if ext in extensions:\n",
    "            pt_filename = hf_hub_download(\n",
    "                model_id, revision=revision, filename=filename, token=token, cache_dir=folder\n",
    "            )\n",
    "            dirname, raw_filename = os.path.split(filename)\n",
    "            if raw_filename == \"pytorch_model.bin\":\n",
    "                # XXX: This is a special case to handle `transformers` and the\n",
    "                # `transformers` part of the model which is actually loaded by `transformers`.\n",
    "                sf_in_repo = os.path.join(dirname, \"model.safetensors\")\n",
    "            else:\n",
    "                sf_in_repo = f\"{prefix}.safetensors\"\n",
    "            sf_filename = os.path.join(folder, sf_in_repo)\n",
    "            try:\n",
    "                convert_file(pt_filename, sf_filename, discard_names=[])\n",
    "                operations.append(CommitOperationAdd(path_in_repo=sf_in_repo, path_or_fileobj=sf_filename))\n",
    "            except Exception as e:\n",
    "                errors.append((pt_filename, e))\n",
    "    return operations, errors\n",
    "\n",
    "\n",
    "def convert(\n",
    "    api: \"HfApi\", model_id: str, revision: Optional[str] = None, force: bool = False\n",
    ") -> Tuple[\"CommitInfo\", List[Tuple[str, \"Exception\"]]]:\n",
    "    pr_title = \"Adding `safetensors` variant of this model\"\n",
    "    info = api.model_info(model_id, revision=revision)\n",
    "    filenames = set(s.rfilename for s in info.siblings)\n",
    "\n",
    "    with TemporaryDirectory() as d:\n",
    "        folder = os.path.join(d, repo_folder_name(repo_id=model_id, repo_type=\"models\"))\n",
    "        os.makedirs(folder)\n",
    "        new_pr = None\n",
    "        try:\n",
    "            operations = None\n",
    "            pr = previous_pr(api, model_id, pr_title, revision=revision)\n",
    "\n",
    "            library_name = getattr(info, \"library_name\", None)\n",
    "            if any(filename.endswith(\".safetensors\") for filename in filenames) and not force:\n",
    "                raise AlreadyExists(f\"Model {model_id} is already converted, skipping..\")\n",
    "            elif pr is not None and not force:\n",
    "                url = f\"https://huggingface.co/{model_id}/discussions/{pr.num}\"\n",
    "                new_pr = pr\n",
    "                raise AlreadyExists(f\"Model {model_id} already has an open PR check out {url}\")\n",
    "            elif library_name == \"transformers\":\n",
    "\n",
    "                discard_names = get_discard_names(model_id, revision=revision, folder=folder, token=api.token)\n",
    "                if \"pytorch_model.bin\" in filenames:\n",
    "                    operations, errors = convert_single(\n",
    "                        model_id, revision=revision, folder=folder, token=api.token, discard_names=discard_names\n",
    "                    )\n",
    "                elif \"pytorch_model.bin.index.json\" in filenames:\n",
    "                    operations, errors = convert_multi(\n",
    "                        model_id, revision=revision, folder=folder, token=api.token, discard_names=discard_names\n",
    "                    )\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Model {model_id} doesn't seem to be a valid pytorch model. Cannot convert\")\n",
    "            else:\n",
    "                operations, errors = convert_generic(\n",
    "                    model_id, revision=revision, folder=folder, filenames=filenames, token=api.token\n",
    "                )\n",
    "\n",
    "            if operations:\n",
    "                new_pr = api.create_commit(\n",
    "                    repo_id=model_id,\n",
    "                    revision=revision,\n",
    "                    operations=operations,\n",
    "                    commit_message=pr_title,\n",
    "                    commit_description=COMMIT_DESCRIPTION,\n",
    "                    create_pr=True,\n",
    "                )\n",
    "                print(f\"Pr created at {new_pr.pr_url}\")\n",
    "            else:\n",
    "                print(\"No files to convert\")\n",
    "        finally:\n",
    "            shutil.rmtree(folder)\n",
    "        return new_pr, errors\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DESCRIPTION = \"\"\"\n",
    "    Simple utility tool to convert automatically some weights on the hub to `safetensors` format.\n",
    "    It is PyTorch exclusive for now.\n",
    "    It works by downloading the weights (PT), converting them locally, and uploading them back\n",
    "    as a PR on the hub.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=DESCRIPTION)\n",
    "    parser.add_argument(\n",
    "        \"model_id\",\n",
    "        type=str,\n",
    "        help=\"The name of the model on the hub to convert. E.g. `gpt2` or `facebook/wav2vec2-base-960h`\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        help=\"The revision to convert\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--force\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Create the PR even if it already exists of if the model was already converted.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-y\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Ignore safety prompt\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    model_id = args.model_id\n",
    "    api = HfApi()\n",
    "    if args.y:\n",
    "        txt = \"y\"\n",
    "    else:\n",
    "        txt = input(\n",
    "            \"This conversion script will unpickle a pickled file, which is inherently unsafe. If you do not trust this file, we invite you to use\"\n",
    "            \" https://huggingface.co/spaces/safetensors/convert or google colab or other hosted solution to avoid potential issues with this file.\"\n",
    "            \" Continue [Y/n] ?\"\n",
    "        )\n",
    "    if txt.lower() in {\"\", \"y\"}:\n",
    "        commit_info, errors = convert(api, model_id, revision=args.revision, force=args.force)\n",
    "        string = f\"\"\"\n",
    "### Success 🔥\n",
    "Yay! This model was successfully converted and a PR was open using your token, here:\n",
    "[{commit_info.pr_url}]({commit_info.pr_url})\n",
    "        \"\"\"\n",
    "        if errors:\n",
    "            string += \"\\nErrors during conversion:\\n\"\n",
    "            string += \"\\n\".join(\n",
    "                f\"Error while converting {filename}: {e}, skipped conversion\" for filename, e in errors\n",
    "            )\n",
    "        print(string)\n",
    "    else:\n",
    "        print(f\"Answer was `{txt}` aborting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y114sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m safetensors_conversion\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/pre_train_gpt_infini_v2.ipynb#Y114sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m safetensors_conversion(\u001b[39m\"\u001b[39;49m\u001b[39mresults/models_baseline/GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I\u001b[39;49m\u001b[39m\"\u001b[39;49m, force\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
