{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training GPT-Neo on TinyStories\n",
    "HuggingFace links: [GPT-Neo model](https://huggingface.co/docs/transformers/model_doc/gpt_neo), [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wandb\n",
    "# %pip install transformers\n",
    "# %pip install datasets\n",
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkeskull\u001b[0m (\u001b[33mlkeskullorg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/laurikeskull/.netrc\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri common/pre_train_infini_gpt.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri%20common/pre_train_infini_gpt.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri%20common/pre_train_infini_gpt.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2TokenizerFast, GPTNeoForCausalLM, GPTNeoConfig\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri%20common/pre_train_infini_gpt.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri%20common/pre_train_infini_gpt.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset, load_from_disk, DatasetDict\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri%20common/pre_train_infini_gpt.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m debug_mode \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/utils/import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1499\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1500\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1501\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1502\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/utils/import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1509\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1511\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1512\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1513\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:41\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n\u001b[1;32m     39\u001b[0m \u001b[39m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mintegrations\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     43\u001b[0m     hp_params,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39m# isort: on\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/utils/import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1499\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1500\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1501\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1502\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/utils/import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1509\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1511\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1512\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1513\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1069\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:729\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('keys.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "import wandb; wandb.login(key=api_key)\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPTNeoForCausalLM, GPTNeoConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "debug_mode = True\n",
    "config = GPTNeoConfig(\n",
    "    vocab_size=10_000,\n",
    "    hidden_size=512,\n",
    "    max_position_embeddings=512,\n",
    "    num_layers=2,\n",
    "    attention_types=[[[\"global\", \"local\"], 1]],\n",
    "    num_heads=4,\n",
    "    window_size=256,\n",
    "    intermediate_size=1024\n",
    ")\n",
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "All experiments use the same tokenizer, so in theory, we only need to preprocess our data once. This means you can subsequently skip this CPU-intensive task on the DelftBlue GPU nodes. \n",
    "(except if you are experimenting with different context lengths, in that case you will need to re-tokenise to recover potentially truncated inputs; see below).\n",
    "You may even have to load the entire dataset (or pre-load chunks) into memory, DelftBlue has extremely slow IO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation\n",
    "Two things happen during tokenisation, of which the latter is optional:\n",
    "\n",
    "1. Map (sub-)words to integers $ \\in [0, 10000]$.\n",
    "2. Truncate/pad each input as necessary to fit within `hidden_size`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding token is <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('10k-gpt-neo', model_max_length=config.hidden_size)\n",
    "\n",
    "# in theory, window_size x num_layers is possible. \n",
    "# But, this project is not about the efficacy of sliding window attention (though it could be!)\n",
    "assert tokenizer.model_max_length == 512 \n",
    "assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "# printing this because of a bug in tokenizers (should be fixed now) https://github.com/huggingface/transformers/issues/26500\n",
    "print(f'padding token is {tokenizer.pad_token}')\n",
    "# HF wasn't saving this nor the tokenizer's pad_token\n",
    "config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 1000/1000 [00:00<00:00, 8537.08 examples/s] \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Saving the dataset (5/5 shards): 100%|██████████| 100/100 [00:00<00:00, 1044.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('roneneldan/tinystories')\n",
    "\n",
    "small_dataset = DatasetDict({\n",
    "    'train': dataset['train'].select(range(1000)),\n",
    "    'validation': dataset['validation'].select(range(100))\n",
    "})\n",
    "\n",
    "if debug_mode:\n",
    "    dataset = small_dataset\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), \n",
    "    batched=True, num_proc=8, batch_size=1_000)\n",
    "\n",
    "tokenized_dataset.save_to_disk(f'./tokenized_dataset', num_proc=5)\n",
    "\n",
    "tokenized_dataset = load_from_disk(f'./tokenized_dataset')\n",
    "\n",
    "assert len(tokenized_dataset['train'][0]['input_ids']) == config.hidden_size\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:] \n",
    "# should be eos tokens (9999), given that most short stories are <512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### Model\n",
    "We use GPT-Neo as the architecture for our model. This consists of the following blocks, generally seen in transformer architectures:\n",
    "\n",
    "- Embeddings: Map one-hot (sparse) token vectors to a dense vector of length `hidden_size`. Add positional encoding. \n",
    "- $n$ Blocks: Contain self-attention and FFNs.\n",
    "- Head: Map hidden state back to a useful output. \n",
    "\n",
    "We use the config specified at the top of this notebook to construct a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTNeoSelfAttention(nn.Module):\n",
    "    def __init__(self, config, attention_type):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        max_positions = config.max_position_embeddings\n",
    "        bias = torch.tril(torch.ones((max_positions, max_positions), dtype=bool)).view(\n",
    "            1, 1, max_positions, max_positions\n",
    "        )\n",
    "\n",
    "        # local causal self attention is a sliding window where each token can only attend to the previous\n",
    "        # window_size tokens. This is implemented by updating the causal mask such that for each token\n",
    "        # all other tokens are masked except the previous window_size tokens.\n",
    "        if attention_type == \"local\":\n",
    "            bias = torch.bitwise_xor(bias, torch.tril(bias, -config.window_size))\n",
    "\n",
    "        self.register_buffer(\"bias\", bias, persistent=False)\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e9), persistent=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(float(config.attention_dropout))\n",
    "        self.resid_dropout = nn.Dropout(float(config.resid_dropout))\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n",
    "                f\" {self.num_heads}).\"\n",
    "            )\n",
    "        \n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "        self.beta = nn.Parameter(torch.zeros(1,))\n",
    "        self.ELU = nn.ELU()\n",
    "\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Splits hidden_size dim into attn_head_size and num_heads\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        \"\"\"\n",
    "        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def _attn(self, query, key, value, compressive_memory, z, attention_mask=None, head_mask=None):\n",
    "        # Keep the attention weights computation in fp32 to avoid overflow issues\n",
    "        query = query.to(torch.float32)\n",
    "        key = key.to(torch.float32)\n",
    "\n",
    "        sigma_q = self.ELU(query[:, :, :, :]) + 1.0\n",
    "        sigma_k = self.ELU(key[:, :, :, :]) + 1.0\n",
    "\n",
    "        A_mem = ((sigma_q @ compressive_memory) / ((sigma_q @ z) + 1e-6))\n",
    "        A_dot = query[:, :, :, :] @ key[:, :, :, :].transpose(-2, -1)\n",
    "\n",
    "        query_length, key_length = query.size(-2), key.size(-2)\n",
    "        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "        mask_value = torch.finfo(A_dot.dtype).min\n",
    "        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "        mask_value = torch.tensor(mask_value, dtype=A_dot.dtype).to(A_dot.device)\n",
    "        A_dot = torch.where(causal_mask, A_dot, mask_value)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask\n",
    "            A_dot = A_dot + attention_mask\n",
    "\n",
    "        # attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        A_dot = A_dot.to(value.dtype)\n",
    "        A_dot = self.attn_dropout(A_dot)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            A_dot = A_dot * head_mask\n",
    "            \n",
    "        A_dot = nn.functional.softmax(A_dot / torch.sqrt(torch.tensor(self.head_dim)), dim = -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        A_dot = torch.matmul(A_dot, value)\n",
    "        attention_output = (F.sigmoid(self.beta) * A_mem) + ((1 - F.sigmoid(self.beta)) * A_dot)\n",
    "\n",
    "        delta = (sigma_k @ compressive_memory) / ((sigma_k @ z) + 1e-6)\n",
    "        compressive_memory = compressive_memory + (sigma_k.transpose(-2, -1) @ (value[:, :, :, :] - delta))\n",
    "\n",
    "        z = z + sigma_k.sum(dim = -2, keepdim = True)\n",
    "\n",
    "        return attention_output, A_dot\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "        device = query.device\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        compressive_memory = torch.zeros((self.num_heads, self.head_dim, self.head_dim), device=device)\n",
    "        z = torch.zeros((self.num_heads, self.head_dim, 1), device=device)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key = layer_past[0]\n",
    "            past_value = layer_past[1]\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        attn_output, attn_weights = self._attn(query, key, value, compressive_memory, z, attention_mask, head_mask)\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "\n",
    "        return outputs  # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,585,666 parameters.\n"
     ]
    }
   ],
   "source": [
    "class CustomGPTNeoForCausalLM(GPTNeoForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Assuming attention types are specified in config.attention_layers\n",
    "        # Replace the standard attention with the specified type\n",
    "        for block in self.transformer.h:\n",
    "            block.attn.attention = GPTNeoSelfAttention(config, block.attn.attention_type)\n",
    "model = CustomGPTNeoForCausalLM( \n",
    "    config = config,\n",
    ")\n",
    "print(f'The model has {model.num_parameters():,} parameters.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = CustomGPTNeoForCausalLM( \n",
    "# #     config = config,\n",
    "# # )\n",
    "# model = GPTNeoForCausalLM(config=config)\n",
    "\n",
    "# print(f'The model has {model.num_parameters():,} parameters.')\n",
    "# # Token embeddings:    10'000 * 512 = 5M \n",
    "# # Position embeddings:    512 * 512 = 260K\n",
    "# # 2 Blocks: 2 * (~1M + ~1M) = 4M\n",
    "# #   Attention: 4 * 512 * 512  (4 because of k, q, v, o)     = 1M \n",
    "# #   FFNs:      2 * 512 * 1024 (c_fc, c_proj)                = 1M \n",
    "\n",
    "# # total: ~9.5M \n",
    "\n",
    "# # note that the above does not account for the LM_head, which typically gets swapped out depending on the downstream task. \n",
    "# # the LM_head, in this case, maps logits back to tokens\n",
    "# # 512 * 10'000 = 5M    (A decent chunk compared to our 9.5M parameters)\n",
    "\n",
    "# # NOTE: uncomment to see the model architecture \n",
    "# # model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison. Note, wte should be (10000, 64)\n",
    "# GPTNeoForCausalLM.from_pretrained('roneneldan/TinyStories-1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "Huggingface provides some powerful (and often confusingly long) APIs for model training. The `TrainingArguments` specifies our hyperparameters, which are used by the `Trainer` taking in the remaining objects (like `model`, `tokenizer`, and `train_dataset`). Specifically:\n",
    "\n",
    "- `learning_rate` and `num_train_epochs` determine how much the model learns. A higher rate is faster, but more unstable. More epochs (entire passes over the dataset) yields incrementally better results, at the cost of more training time. \n",
    "- Batch sizes determine how many samples the model sees in *parallel*. Given `gradient_accumulation_steps=1` and a `batch_size=8`, the model will backpropagate the average loss of 8 samples; if `batch_size=1`, it will average the loss of `gradient_accumulation_steps` samples. It is important to make sure the backpropagated loss is averaged over the same number of samples, when comparing models. \n",
    "\n",
    "- `data_collator` batches (and optionally, pads) the input for the model. We have already padded in our `tokenized_dataset`, and leaving this argument empty will automatically batch the inputs. So why do we need it? \n",
    "\n",
    "    Glad you asked. This has to do with how the loss is computed in causal language modelling. In our case, we try to predict $p(y | x)$, where $x$ is an input sequence of tokens, and $y$ is the next token following that sequence. Our model, unaware of the target token $y$, outputs $\\hat y$. \n",
    "    \n",
    "    For `Trainer` to compute the (cross-entropy) loss, we need to provide it with both $y$ and $\\hat y$. The `DataCollatorForLanguageModeling` knows this, and provides the next token $y$ as a separate part of the input, to the `Trainer`.\n",
    "\n",
    "    The loss is the backbone of backpropagation, which we need to actually improve our model. If this is confusing, please re-watch Karpathy's GPT tutorial. \n",
    "\n",
    "If you prefer to write the training loop yourself, check out HF' `run_clm_no_trainer.py` scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = tokenized_dataset['train'], tokenized_dataset['validation']\n",
    "\n",
    "batch_size = 16 # TinyStories claims 80, but I am training locally on my poor M1 Air\n",
    "num_train_epochs = 1  # TinyStories doesn't mention\n",
    "gradient_accumulation_steps = 16 # TinyStories claims 16\n",
    "\n",
    "lr = 5e-4 # TinyStories claims 5e-4, higher values are preferable for smaller models\n",
    "\n",
    "_train_steps = len(train_dataset) // (batch_size * gradient_accumulation_steps)\n",
    "eval_steps = _train_steps // 10  # Evaluate every 10% of training steps\n",
    "\n",
    "\n",
    "model_name = f'{model.num_parameters()//1e6:.1f}M-{config.num_layers}L-{config.num_heads}H-{config.hidden_size}C-{config.intermediate_size}I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    training for 1 epochs, 1000 samples\n",
      "    16 batch size, 16 accumulation steps\n",
      "    gives 3 training steps.\n",
      "\n",
      "    evaluating every 0 steps, 100 samples \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    seed       = 42,\n",
    "    use_cpu    = False, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "    output_dir = f'./results/models/{model_name}',\n",
    "\n",
    "    # NOTE: training params\n",
    "    learning_rate    = lr,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    # Use a smaller batch size to fit into GPU RAM. \n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size  = batch_size,\n",
    "    # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "    # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "    # NOTE: Evaluation params\n",
    "    # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = eval_steps,\n",
    "    save_steps = eval_steps,\n",
    "\n",
    "    logging_first_step=True,\n",
    "    logging_steps=max(1,eval_steps),\n",
    "    report_to  = 'wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset = train_dataset, \n",
    "    eval_dataset = eval_dataset,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# print amount of training steps, and how often the model is evaluated\n",
    "print(f'''\n",
    "    training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "    {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "    gives {_train_steps} training steps.\n",
    "\n",
    "    evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train. \n",
    "\n",
    "This configuration takes ≤24hr to pre-train on my M1 Macbook Air with 16GB RAM. Python takes ≤4GB VRAM at a `batch_size=16` and ≤11GB at `batch_size=64`, though they take the same amount of time to train - likely because this processor is not designed to move that much data in and out of RAM constantly. And tbh, the GPU be lacking. If you decide to go the local-training-route, consider [chai](https://github.com/lvillani/chai) to keep your (Apple) laptop awake – there's probably a windows/linux equivalent too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [05:25<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:47<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.3204, 'grad_norm': 4.091865539550781, 'learning_rate': 0.0003333333333333333, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                             \n",
      "  0%|          | 0/3 [00:49<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.472726821899414, 'eval_runtime': 1.6637, 'eval_samples_per_second': 60.106, 'eval_steps_per_second': 4.207, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [01:46<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4734, 'grad_norm': 2.7479536533355713, 'learning_rate': 0.00016666666666666666, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                             \n",
      "  0%|          | 0/3 [02:00<?, ?it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.11562728881836, 'eval_runtime': 13.56, 'eval_samples_per_second': 7.375, 'eval_steps_per_second': 0.516, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [02:58<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1156, 'grad_norm': 2.2963554859161377, 'learning_rate': 0.0, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                             \n",
      "  0%|          | 0/3 [03:02<?, ?it/s]\n",
      "\u001b[A\n",
      "100%|██████████| 3/3 [02:31<00:00, 50.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.976396560668945, 'eval_runtime': 4.1233, 'eval_samples_per_second': 24.252, 'eval_steps_per_second': 1.698, 'epoch': 0.76}\n",
      "{'train_runtime': 151.1384, 'train_samples_per_second': 6.616, 'train_steps_per_second': 0.02, 'train_loss': 8.636452674865723, 'epoch': 0.76}\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project='tinystories', name=model_name, config=training_args)\n",
    "trainer.train()\n",
    "trainer.save_model(f'./results/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁</td></tr><tr><td>eval/runtime</td><td>▁█▂</td></tr><tr><td>eval/samples_per_second</td><td>█▁▃</td></tr><tr><td>eval/steps_per_second</td><td>█▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▅▅███</td></tr><tr><td>train/global_step</td><td>▁▁▅▅███</td></tr><tr><td>train/grad_norm</td><td>█▃▁</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>7.9764</td></tr><tr><td>eval/runtime</td><td>4.1233</td></tr><tr><td>eval/samples_per_second</td><td>24.252</td></tr><tr><td>eval/steps_per_second</td><td>1.698</td></tr><tr><td>total_flos</td><td>9917352640512.0</td></tr><tr><td>train/epoch</td><td>0.7619</td></tr><tr><td>train/global_step</td><td>3</td></tr><tr><td>train/grad_norm</td><td>2.29636</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>8.1156</td></tr><tr><td>train_loss</td><td>8.63645</td></tr><tr><td>train_runtime</td><td>151.1384</td></tr><tr><td>train_samples_per_second</td><td>6.616</td></tr><tr><td>train_steps_per_second</td><td>0.02</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">forgotten-emperor-2</strong> at: <a href='https://wandb.ai/lkeskullorg/huggingface/runs/8xtvdfbo' target=\"_blank\">https://wandb.ai/lkeskullorg/huggingface/runs/8xtvdfbo</a><br/> View project at: <a href='https://wandb.ai/lkeskullorg/huggingface' target=\"_blank\">https://wandb.ai/lkeskullorg/huggingface</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240504_214425-8xtvdfbo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference!\n",
    "Try out your own pre-trained model on some prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap # for pretty printing\n",
    "w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = '9.0M-2L-4H-512C-1024I'\n",
    "tokenizer = AutoTokenizer.from_pretrained('10k-gpt-neo')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(f'results/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257]])\n",
      "\n",
      "\u001b[3m   Once upon a time, there was a\n",
      "  a..,,,,,,,,,,,,,,,,,,,,,,,,,,........,,,,,,,,,,,....................................................................................................................................................................................................................................................\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time, there was a'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "output = model.generate(input_ids, max_length=300, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "\n",
    "# textwrap with indentation on every new paragraph\n",
    "see(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
