{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training [`GPT-Neo`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py) & [`RoBERTa`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py) on [`TinyStories`](https://huggingface.co/datasets/roneneldan/TinyStories)\n",
    "\n",
    "This script shows how to pre-train both models. I put them in one notebook because the majority of the code is shared; but you may want to separate the logic per model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30047) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch wandb transformers[torch] datasets tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers \n",
    "print(transformers.__version__)\n",
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # or \"0,1\" for multiple GPUs\n",
    "from typing import Optional, Tuple\n",
    "# import wandb; wandb.login()\n",
    "\n",
    "#wandb login with api key\n",
    "import wandb\n",
    "wandb.login(key=\"b18c2e103bce4f9c38f895080cbdfb6284617f99\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    RobertaForMaskedLM, RobertaConfig, RobertaTokenizer,\n",
    "    GPTNeoForCausalLM, GPTNeoConfig, GPT2TokenizerFast\n",
    ")\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models \n",
    "We consider GPT-Neo and BERT as base transformer architectures. This consists of the following blocks linked via residual connections:\n",
    "\n",
    "- Embeddings: Map one-hot (sparse) token vectors to a dense vector of length `hidden_size`. Add positional encoding. \n",
    "- $n$ Blocks: Contain self-attention and FFNs.\n",
    "- Head: Map hidden state back to a useful output. \n",
    "\n",
    "This specifies some of the model-related hyperparameters. I chose them based on what achieved reasonable performance in the [TinyStories paper](https://arxiv.org/abs/2305.07759), while also being feasible to train on our limited compute budgets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gpt = dict(\n",
    "\n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   # number of tokens in the vocabulary \n",
    "    hidden_size             = 516,      # embedding size (vector length) of each token \n",
    "    max_position_embeddings = 512,      # maximum sequence length (context window)\n",
    "\n",
    "    # BLOCKS (ATTN & FFN)\n",
    "    num_layers          = 2,                    # number of transformer blocks\n",
    "    attention_types     = [[[\"global\", \"local\"], 1]], # (GPT-Neo-specific) global and local attention \n",
    "    num_heads           = 4,                    # attention heads\n",
    "    window_size         = 256,                  # (GPT-Neo-specific) for local attention \n",
    "    intermediate_size   = 1024,                 # size of 'up-projection' layer in FFN\n",
    "\n",
    "    pad_token_id = 0,           # need to specify this for tokenizer interop between models\n",
    "    segment_len = 16,           # (InfiniAttention) \n",
    ")\n",
    "\n",
    "config_rob = dict(\n",
    "    \n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   \n",
    "    hidden_size             = 516,      \n",
    "    # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "    max_position_embeddings = config_gpt['max_position_embeddings'] + 1,\n",
    "\n",
    "    # BLOCKS (of course naming is different in roberta :) )\n",
    "    num_hidden_layers = config_gpt['num_layers'],\n",
    "    num_attention_heads = config_gpt['num_heads'],\n",
    "    intermediate_size=1024,                     \n",
    "\n",
    "    pad_token_id = 0,\n",
    ")\n",
    "\n",
    "config_gpt = GPTNeoConfig(**config_gpt)\n",
    "config_rob = RobertaConfig(**config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selfattentionVanilla import GPTNeoSelfAttentionVanilla\n",
    "from selfattention import InfiniAttention\n",
    "class InfiniAttentionGPTNeoForCausalLM(GPTNeoForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Assuming attention types are specified in config.attention_layers\n",
    "        # Replace the standard attention with the specified type\n",
    "        for block in self.transformer.h:\n",
    "            block.attn.attention = InfiniAttention(config_gpt, block.attn.attention_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This GPT has 9,677,056 parameters,\n",
      "     and ROB has 9,959,496 parameters.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30048) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# TODO: you should set all pytorch & huggingface seeds here as model initialisation depends on it\n",
    "\n",
    "gpt = InfiniAttentionGPTNeoForCausalLM(config=config_gpt)\n",
    "rob = RobertaForMaskedLM(config=config_rob)\n",
    "\n",
    "print(f'''\n",
    "    This GPT has {gpt.num_parameters():,} parameters,\n",
    "     and ROB has {rob.num_parameters():,} parameters.\n",
    "    ''')\n",
    "\n",
    "# gpt, rob # uncomment to see model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is: /Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri\n",
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n",
      "The current directory is: /Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri\n",
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "def get_tokenizer_for_config(Tok: PreTrainedTokenizer, config: PretrainedConfig):\n",
    "    print(f'The current directory is: {os.getcwd()}')\n",
    "    tokenizer = Tok.from_pretrained(\n",
    "        '10k-tok',                                         # our custom tokenizer\n",
    "        model_max_length=config.max_position_embeddings    # sequence length (context window)\n",
    "    )\n",
    "\n",
    "    # we're using our special tokenizer with only 10'000 tokens instead of 50'256\n",
    "    assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "    print(f'padding token is {tokenizer.pad_token}')\n",
    "    print(f'padding token in config: {config.pad_token_id}, in tokeniser: {tokenizer.pad_token_id}')\n",
    "    \n",
    "    return tokenizer \n",
    "\n",
    "tok_gpt = get_tokenizer_for_config(GPT2TokenizerFast, config_gpt)\n",
    "tok_rob = get_tokenizer_for_config(RobertaTokenizer, config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk \n",
    "\n",
    "if debug_mode:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset_small')\n",
    "else:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset')\n",
    "\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset  = tokenized_dataset['validation']\n",
    "\n",
    "assert len(tokenized_dataset['train'][0]['input_ids']) == config_gpt.max_position_embeddings\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:]\n",
    "# should be pad tokens (0), given that most short stories are <512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Before we get started, you may want to specify which GPU to use. See the first cell in this notebook; make sure to run it before anything else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface provides some powerful (and often confusingly long) APIs for model training. The `TrainingArguments` specifies our hyperparameters, which are used by the `Trainer` taking in the remaining objects (like `model`, `tokenizer`, and `train_dataset`). Specifically:\n",
    "\n",
    "- `learning_rate` and `num_train_epochs` determine how much the model learns. A higher rate is faster, but more unstable. More epochs (entire passes over the dataset) yields incrementally better results, at the cost of more training time. \n",
    "- Batch sizes determine how many samples the model sees in *parallel*. Given `gradient_accumulation_steps=1` and a `batch_size=8`, the model will backpropagate the average loss of 8 samples; if `batch_size=1`, it will average the loss of `gradient_accumulation_steps` samples. It is important to make sure the backpropagated loss is averaged over the same number of samples, when comparing models. \n",
    "\n",
    "- `data_collator` batches (and optionally, pads) the input for the model. We have already padded in our `tokenized_dataset`, and leaving this argument empty will automatically batch the inputs. So why do we need it? \n",
    "\n",
    "    Glad you asked. This has to do with how the loss is computed in causal language modelling. In our case, we try to predict $p(y | x)$, where $x$ is an input sequence of tokens, and $y$ is the next token following that sequence. Our model, unaware of the target token $y$, outputs $\\hat y$. \n",
    "    \n",
    "    For `Trainer` to compute the (cross-entropy) loss, we need to provide it with both $y$ and $\\hat y$. The `DataCollatorForLanguageModeling` knows this, and provides the next token $y$ as a separate part of the input, to the `Trainer`.\n",
    "\n",
    "    The loss is the backbone of backpropagation, which we need to actually improve our model. If this is confusing, please re-watch Karpathy's GPT tutorial. \n",
    "\n",
    "If you prefer to write the training loop yourself, check out HF' `run_clm_no_trainer.py` scripts. (`run_mlm_no_trainer.py` for RoBERTa-style masked-language modelling, as opposed to causal language modelling). This can be useful to give you better control over which devices are used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "model_name_for_save = ''\n",
    "def get_hyperparameters(model, dataset):\n",
    "    ''' common hyperparameters to give to the trainer '''\n",
    "\n",
    "    # TRAINING HYPERPARAMETERS \n",
    "    batch_size = 16                  # TinyStories uses 80, but I am training locally on my poor M1 Air\n",
    "    num_train_epochs = 1             # TinyStories doesn't mention\n",
    "    gradient_accumulation_steps = 16 # TinyStories uses 16\n",
    "\n",
    "    lr = 5e-4                        # TinyStories uses 5e-4, higher values better for small models\n",
    "\n",
    "    # future you will thank you for descriptive model names\n",
    "    # TODO: customise this name such that every model you train has a unique identifier!\n",
    "    config      = model.config \n",
    "    model_name  = '-'.join([\n",
    "        'INFINI_GPT' if isinstance(model, InfiniAttentionGPTNeoForCausalLM) else (\n",
    "        'GPT' if isinstance(model, GPTNeoForCausalLM) else 'BERT'),\n",
    "        f'{model.num_parameters()//1e6:.1f}M',\n",
    "        f'{config.num_layers if isinstance(model, GPTNeoForCausalLM) else config.num_hidden_layers}L', \n",
    "        f'{config.num_heads if isinstance(model, GPTNeoForCausalLM) else config.num_attention_heads}H', \n",
    "        f'{config.hidden_size}C',\n",
    "        f'{config.intermediate_size}I'\n",
    "    ])\n",
    "    model_name_for_save = model_name\n",
    "\n",
    "    _train_steps = len(dataset) // (batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = _train_steps // 10 # evaluate every 10% of training steps\n",
    "\n",
    "    return dict(\n",
    "        model_name = model_name,\n",
    "        batch_size = batch_size, \n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        lr = lr,\n",
    "        eval_steps = eval_steps\n",
    "    )\n",
    "\n",
    "params_gpt = get_hyperparameters(gpt, train_dataset)\n",
    "params_rob = get_hyperparameters(rob, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(\n",
    "        model, tokenizer, train_dataset, eval_dataset, output_dir,\n",
    "        model_name, batch_size, num_train_epochs, gradient_accumulation_steps, lr, eval_steps):\n",
    "    ''' more general training arguments you likely want to keep fixed'''\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "\n",
    "        seed       = 42,\n",
    "        # use_cpu    = True, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "\n",
    "        output_dir = os.path.join(output_dir, model_name),\n",
    "\n",
    "        # NOTE: training params\n",
    "        learning_rate    = lr,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        # Use a smaller batch size to fit into GPU RAM. \n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size  = batch_size,\n",
    "        # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "        # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "        # NOTE: Evaluation params\n",
    "        # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "\n",
    "        logging_first_step=True,\n",
    "        logging_steps=100,\n",
    "        report_to  = 'wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset, \n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=isinstance(model, RobertaForMaskedLM)),\n",
    "    )\n",
    "\n",
    "    # print amount of training steps, and how often the model is evaluated\n",
    "    print(f'''\n",
    "    Retrieving Trainer for \\033[1m{model_name}\\033[0m\n",
    "        training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "        {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "        gives {len(train_dataset)//(batch_size * gradient_accumulation_steps)} training steps.\n",
    "        Evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "        ''')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30049) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30050) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(30052) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Retrieving Trainer for \u001b[1mINFINI_GPT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 100 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 0 training steps.\n",
      "        Evaluating every 0 steps, 10 samples \n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30053) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Retrieving Trainer for \u001b[1mBERT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 100 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 0 training steps.\n",
      "        Evaluating every 0 steps, 10 samples \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "out_dir = './results/models_baseline/' \n",
    "\n",
    "trainer_gpt = get_trainer(gpt, tok_gpt, train_dataset, eval_dataset, out_dir, **params_gpt)\n",
    "trainer_rob = get_trainer(rob, tok_rob, train_dataset, eval_dataset, out_dir, **params_rob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train. \n",
    "\n",
    "This configuration takes ≤24hr to pre-train on my M1 Macbook Air with 16GB RAM. Python takes ≤4GB VRAM at a `batch_size=16` and ≤11GB at `batch_size=64`, though they take the same amount of time to train - likely because this processor is not designed to move that much data in and out of RAM constantly. And tbh, the GPU be lacking. If you decide to go the local-training-route, consider [chai](https://github.com/lvillani/chai) to keep your (Apple) laptop awake – there's probably a windows/linux equivalent too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(trainer: Trainer, name: str, out_dir: str): \n",
    "\n",
    "    wandb.init(project='tiny-transformers', name=name, group='baseline', config=trainer.args)\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(out_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30055) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# do_train(trainer_rob, params_rob['model_name'], out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(30057) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/wandb/run-20240517_113933-izirag2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lkeskullorg/tiny-transformers/runs/izirag2q' target=\"_blank\">INFINI_GPT-9.0M-2L-4H-516C-1024I</a></strong> to <a href='https://wandb.ai/lkeskullorg/tiny-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lkeskullorg/tiny-transformers' target=\"_blank\">https://wandb.ai/lkeskullorg/tiny-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lkeskullorg/tiny-transformers/runs/izirag2q' target=\"_blank\">https://wandb.ai/lkeskullorg/tiny-transformers/runs/izirag2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0633, 'grad_norm': 1.3105493783950806, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 9.9787, 'train_samples_per_second': 10.021, 'train_steps_per_second': 0.1, 'train_loss': 4.063251495361328, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "do_train(trainer_gpt, params_gpt['model_name'], out_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(26049) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right after training\n",
    "trained_model = trainer_gpt.model\n",
    "model_name = params_gpt['model_name']\n",
    "\n",
    "torch.save(trained_model.state_dict(), f'./results/models_baseline/{model_name}/model_state.pt')\n",
    "\n",
    "# before evaluation\n",
    "config_1 = GPTNeoConfig.from_pretrained(f'./results/models_baseline/{model_name}')\n",
    "custom_model_1 = InfiniAttentionGPTNeoForCausalLM(config=config_1)\n",
    "trained_state_dict_1 = torch.load(f'./results/models_baseline/{model_name}/model_state.pt')\n",
    "custom_model_1.load_state_dict(trained_state_dict_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your Pre-Trained Model \n",
    "Try out your own model on some prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(23767) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# I made a small function for pretty printing into paragraphs with line wrapping for readability\n",
    "import textwrap \n",
    "w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(24827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Some weights of the model checkpoint at results/models_baseline/INFINI_GPT-9.0M-2L-4H-516C-1024I were not used when initializing InfiniAttentionGPTNeoForCausalLM: ['transformer.h.0.attn.attention.biass', 'transformer.h.1.attn.attention.biass']\n",
      "- This IS expected if you are initializing InfiniAttentionGPTNeoForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing InfiniAttentionGPTNeoForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of InfiniAttentionGPTNeoForCausalLM were not initialized from the model checkpoint at results/models_baseline/INFINI_GPT-9.0M-2L-4H-516C-1024I and are newly initialized: ['transformer.h.0.attn.attention.betas', 'transformer.h.1.attn.attention.betas']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.9247]],\n",
      "\n",
      "         [[-2.1131]],\n",
      "\n",
      "         [[-2.6322]],\n",
      "\n",
      "         [[-0.0114]]]])\n",
      "<class '__main__.InfiniAttentionGPTNeoForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = 'GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I'\n",
    "vanilla = 'GPT-9.0M-2L-4H-516C-1024I'\n",
    "# model_name = 'BERT-9.0M-2L-4H-516C-1024I' # bert won't work for generation unless you fine-tune it for that task\n",
    "infin_name = 'INFINI_GPT-9.0M-2L-4H-516C-1024I'\n",
    "tokenizer = AutoTokenizer.from_pretrained('10k-tok')\n",
    "# tokenizer_old = AutoTokenizer.from_pretrained('10k-gpt-neo')\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = InfiniAttentionGPTNeoForCausalLM.from_pretrained(f'results/models_baseline/{infin_name}', use_safetensors=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(f'results/models_baseline/{infin_name}')\n",
    "state_dict = model.state_dict()\n",
    "print(state_dict['transformer.h.0.attn.attention.betas'])\n",
    "# custom_model.load_state_dict(state_dict)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors in the safetensors file:\n",
      "transformer.h.0.attn.attention.betas\n",
      "transformer.h.0.attn.attention.k_proj.weight\n",
      "transformer.h.0.attn.attention.out_proj.bias\n",
      "transformer.h.0.attn.attention.out_proj.weight\n",
      "transformer.h.0.attn.attention.q_proj.weight\n",
      "transformer.h.0.attn.attention.v_proj.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.1.attn.attention.betas\n",
      "transformer.h.1.attn.attention.k_proj.weight\n",
      "transformer.h.1.attn.attention.out_proj.bias\n",
      "transformer.h.1.attn.attention.out_proj.weight\n",
      "transformer.h.1.attn.attention.q_proj.weight\n",
      "transformer.h.1.attn.attention.v_proj.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.ln_f.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.wpe.weight\n",
      "transformer.wte.weight\n",
      "Tensor transformer.h.0.attn.attention.betas:\n",
      "tensor([[[ 0.6923]],\n",
      "\n",
      "        [[-2.3368]],\n",
      "\n",
      "        [[ 0.6666]],\n",
      "\n",
      "        [[ 2.0836]]])\n",
      "torch.Size([1, 4, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(26088) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "import torch\n",
    "\n",
    "# Path to your .safetensors file\n",
    "safetensors_file_path = 'results/models_baseline/INFINI_GPT-9.0M-2L-4H-516C-1024I/model.safetensors'\n",
    "trained_model_safetensor_path = 'results/infini_baseline/checkpoint-8280/model.safetensors'\n",
    "\n",
    "\n",
    "# Open the safetensors file\n",
    "with safe_open(safetensors_file_path, framework=\"pt\") as f:\n",
    "    # List all tensor names\n",
    "    tensor_names = f.keys()\n",
    "    print(\"Tensors in the safetensors file:\")\n",
    "    for name in tensor_names:\n",
    "        print(name)\n",
    "\n",
    "    # Load a specific tensor\n",
    "    tensor_name = tensor_names[0]  # Replace with the name of the tensor you want to load\n",
    "    tensor = f.get_tensor(tensor_name)\n",
    "    print(f\"Tensor {tensor_name}:\")\n",
    "    print(tensor[0])\n",
    "    print(tensor.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference \n",
    "Let's generate a short story like the ones the model has been trained on! You'll notice that the prompt is surrounded by `<s>`, the begin-of-sequence (bos) token, and `</s>` end-of-sequence (eos) / separator (sep) token. This is from the BERT-style tokenisation, making it clear to the model where (one of several) input sequences ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(24819) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m   <s>Once upon a time, there was a little car named Beep.\n",
      "  He then\n",
      "  </s>.........................................................................................................................................................................................................................................................................................\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 7454, 2402,  257,  640,   11,  612,  373,  257, 1310, 1097, 3706,\n",
       "         1355,  538,   13,  679,  788,  220,    2,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
       "           13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'Once upon a time, there was a little car named Beep. He then '\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "output = custom_model_1.generate(\n",
    "    input_ids,                              # input to the model\n",
    "    max_length=300,                         # maximum generation length\n",
    "    eos_token_id=tokenizer.eos_token_id,    # early stopping when eos_token is output\n",
    "\n",
    "    # num_beams=1,                            # number of beams to use in generation\n",
    "    temperature=1,\n",
    ")\n",
    "# print(output)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "\n",
    "# textwrap with indentation on every new paragraph\n",
    "see(output_text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds like me after a few beers too many, but at least the grammar is (mostly) correct. The model also learns some basic reasoning-like associations like being 'so high' allows you to see 'the whole world'. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
