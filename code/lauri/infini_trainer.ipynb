{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training [`GPT-Neo`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py) & [`RoBERTa`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py) on [`TinyStories`](https://huggingface.co/datasets/roneneldan/TinyStories)\n",
    "\n",
    "This script shows how to pre-train both models. I put them in one notebook because the majority of the code is shared; but you may want to separate the logic per model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch wandb transformers[torch] datasets tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import transformers \n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlkeskull\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/laurikeskull/.netrc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # or \"0,1\" for multiple GPUs\n",
    "from typing import Optional, Tuple\n",
    "# import wandb; wandb.login()\n",
    "\n",
    "#wandb login with api key\n",
    "import wandb\n",
    "wandb.login(key=\"b18c2e103bce4f9c38f895080cbdfb6284617f99\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    RobertaForMaskedLM, RobertaConfig, RobertaTokenizer,\n",
    "    GPTNeoForCausalLM, GPTNeoConfig, GPT2TokenizerFast\n",
    ")\n",
    "from modeling_gptneo import InfiniAttentionGPTNeoForCausalLM\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models \n",
    "We consider GPT-Neo and BERT as base transformer architectures. This consists of the following blocks linked via residual connections:\n",
    "\n",
    "- Embeddings: Map one-hot (sparse) token vectors to a dense vector of length `hidden_size`. Add positional encoding. \n",
    "- $n$ Blocks: Contain self-attention and FFNs.\n",
    "- Head: Map hidden state back to a useful output. \n",
    "\n",
    "This specifies some of the model-related hyperparameters. I chose them based on what achieved reasonable performance in the [TinyStories paper](https://arxiv.org/abs/2305.07759), while also being feasible to train on our limited compute budgets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gpt = dict(\n",
    "\n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   # number of tokens in the vocabulary \n",
    "    hidden_size             = 516,      # embedding size (vector length) of each token \n",
    "    max_position_embeddings = 64,      # maximum sequence length (context window)\n",
    "\n",
    "    # BLOCKS (ATTN & FFN)\n",
    "    num_layers          = 2,                    # number of transformer blocks\n",
    "    attention_types     = [[[\"global\", \"local\"], 1]], # (GPT-Neo-specific) global and local attention \n",
    "    num_heads           = 4,                    # attention heads\n",
    "    window_size         = 256,                  # (GPT-Neo-specific) for local attention \n",
    "    intermediate_size   = 1024,                 # size of 'up-projection' layer in FFN\n",
    "\n",
    "    pad_token_id = 0,           # need to specify this for tokenizer interop between models\n",
    "    segment_len = 16,           # (InfiniAttention) \n",
    ")\n",
    "\n",
    "config_rob = dict(\n",
    "    \n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   \n",
    "    hidden_size             = 516,      \n",
    "    # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "    max_position_embeddings = config_gpt['max_position_embeddings'] + 1,\n",
    "\n",
    "    # BLOCKS (of course naming is different in roberta :) )\n",
    "    num_hidden_layers = config_gpt['num_layers'],\n",
    "    num_attention_heads = config_gpt['num_heads'],\n",
    "    intermediate_size=1024,                     \n",
    "\n",
    "    pad_token_id = 0,\n",
    ")\n",
    "\n",
    "config_gpt = GPTNeoConfig(**config_gpt)\n",
    "config_rob = RobertaConfig(**config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selfattentionVanilla import GPTNeoSelfAttentionVanilla\n",
    "# # from selfattention import InfiniAttention\n",
    "# from old_infini_attention import InfiniAttention\n",
    "# class InfiniAttentionGPTNeoForCausalLM(GPTNeoForCausalLM):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         # Assuming attention types are specified in config.attention_layers\n",
    "#         # Replace the standard attention with the specified type\n",
    "#         for block in self.transformer.h:\n",
    "#             block.attn.attention = InfiniAttention(config_gpt, block.attn.attention_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This GPT has 9,412,864 parameters,\n",
      "     and ROB has 9,728,328 parameters.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: you should set all pytorch & huggingface seeds here as model initialisation depends on it\n",
    "\n",
    "gpt = InfiniAttentionGPTNeoForCausalLM(config=config_gpt)\n",
    "rob = RobertaForMaskedLM(config=config_rob)\n",
    "\n",
    "print(f'''\n",
    "    This GPT has {gpt.num_parameters():,} parameters,\n",
    "     and ROB has {rob.num_parameters():,} parameters.\n",
    "    ''')\n",
    "\n",
    "# gpt, rob # uncomment to see model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory is: /Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri\n",
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n",
      "The current directory is: /Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri\n",
      "padding token is <pad>\n",
      "padding token in config: 0, in tokeniser: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, PretrainedConfig\n",
    "\n",
    "def get_tokenizer_for_config(Tok: PreTrainedTokenizer, config: PretrainedConfig):\n",
    "    print(f'The current directory is: {os.getcwd()}')\n",
    "    tokenizer = Tok.from_pretrained(\n",
    "        '10k-tok',                                         # our custom tokenizer\n",
    "        model_max_length=config.max_position_embeddings    # sequence length (context window)\n",
    "    )\n",
    "\n",
    "    # we're using our special tokenizer with only 10'000 tokens instead of 50'256\n",
    "    assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "    print(f'padding token is {tokenizer.pad_token}')\n",
    "    print(f'padding token in config: {config.pad_token_id}, in tokeniser: {tokenizer.pad_token_id}')\n",
    "    \n",
    "    return tokenizer \n",
    "\n",
    "tok_gpt = get_tokenizer_for_config(GPT2TokenizerFast, config_gpt)\n",
    "tok_rob = get_tokenizer_for_config(RobertaTokenizer, config_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.0 available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk \n",
    "from SegmentedDataset import SegmentedDataset\n",
    "\n",
    "# dataset = load_dataset('roneneldan/tinystories')\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     lambda x: tokenizer(x['text'], truncation=True, padding='max_length'), \n",
    "#     batched=True, num_proc=8, batch_size=1_000)\n",
    "#     tokenized_dataset.save_to_disk(f'./tokenized_dataset', num_proc=5)\n",
    "\n",
    "if debug_mode:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset_small')\n",
    "else:\n",
    "    tokenized_dataset = load_from_disk(f'./tokenized_dataset')\n",
    "\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset  = tokenized_dataset['validation']\n",
    "# print(tokenized_dataset['train'][0]['input_ids'])\n",
    "# assert len(tokenized_dataset['train'][0]['input_ids']) == config_gpt.max_position_embeddings\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:]\n",
    "# print(train_dataset[0]['input_ids'])\n",
    "# should be pad tokens (0), given that most short stories are <512 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Before we get started, you may want to specify which GPU to use. See the first cell in this notebook; make sure to run it before anything else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface provides some powerful (and often confusingly long) APIs for model training. The `TrainingArguments` specifies our hyperparameters, which are used by the `Trainer` taking in the remaining objects (like `model`, `tokenizer`, and `train_dataset`). Specifically:\n",
    "\n",
    "- `learning_rate` and `num_train_epochs` determine how much the model learns. A higher rate is faster, but more unstable. More epochs (entire passes over the dataset) yields incrementally better results, at the cost of more training time. \n",
    "- Batch sizes determine how many samples the model sees in *parallel*. Given `gradient_accumulation_steps=1` and a `batch_size=8`, the model will backpropagate the average loss of 8 samples; if `batch_size=1`, it will average the loss of `gradient_accumulation_steps` samples. It is important to make sure the backpropagated loss is averaged over the same number of samples, when comparing models. \n",
    "\n",
    "- `data_collator` batches (and optionally, pads) the input for the model. We have already padded in our `tokenized_dataset`, and leaving this argument empty will automatically batch the inputs. So why do we need it? \n",
    "\n",
    "    Glad you asked. This has to do with how the loss is computed in causal language modelling. In our case, we try to predict $p(y | x)$, where $x$ is an input sequence of tokens, and $y$ is the next token following that sequence. Our model, unaware of the target token $y$, outputs $\\hat y$. \n",
    "    \n",
    "    For `Trainer` to compute the (cross-entropy) loss, we need to provide it with both $y$ and $\\hat y$. The `DataCollatorForLanguageModeling` knows this, and provides the next token $y$ as a separate part of the input, to the `Trainer`.\n",
    "\n",
    "    The loss is the backbone of backpropagation, which we need to actually improve our model. If this is confusing, please re-watch Karpathy's GPT tutorial. \n",
    "\n",
    "If you prefer to write the training loop yourself, check out HF' `run_clm_no_trainer.py` scripts. (`run_mlm_no_trainer.py` for RoBERTa-style masked-language modelling, as opposed to causal language modelling). This can be useful to give you better control over which devices are used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "model_name_for_save = ''\n",
    "def get_hyperparameters(model, dataset):\n",
    "    ''' common hyperparameters to give to the trainer '''\n",
    "\n",
    "    # TRAINING HYPERPARAMETERS \n",
    "    batch_size = 16                  # TinyStories uses 80, but I am training locally on my poor M1 Air\n",
    "    num_train_epochs = 1             # TinyStories doesn't mention\n",
    "    gradient_accumulation_steps = 16 # TinyStories uses 16\n",
    "\n",
    "    lr = 5e-4                        # TinyStories uses 5e-4, higher values better for small models\n",
    "\n",
    "    # future you will thank you for descriptive model names\n",
    "    # TODO: customise this name such that every model you train has a unique identifier!\n",
    "    config      = model.config \n",
    "    model_name  = '-'.join([\n",
    "        'NEW_INFINI_GPT' if isinstance(model, InfiniAttentionGPTNeoForCausalLM) else (\n",
    "        'GPT' if isinstance(model, GPTNeoForCausalLM) else 'BERT'),\n",
    "        f'{model.num_parameters()//1e6:.1f}M',\n",
    "        f'{config.num_layers if isinstance(model, GPTNeoForCausalLM) else config.num_hidden_layers}L', \n",
    "        f'{config.num_heads if isinstance(model, GPTNeoForCausalLM) else config.num_attention_heads}H', \n",
    "        f'{config.hidden_size}C',\n",
    "        f'{config.intermediate_size}I'\n",
    "    ])\n",
    "    model_name_for_save = model_name\n",
    "\n",
    "    _train_steps = len(dataset) // (batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = _train_steps // 10 # evaluate every 10% of training steps\n",
    "\n",
    "    return dict(\n",
    "        model_name = model_name,\n",
    "        batch_size = batch_size, \n",
    "        num_train_epochs = num_train_epochs,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        lr = lr,\n",
    "        eval_steps = eval_steps\n",
    "    )\n",
    "\n",
    "params_gpt = get_hyperparameters(gpt, train_dataset)\n",
    "params_rob = get_hyperparameters(rob, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(\n",
    "        model, tokenizer, train_dataset, eval_dataset, output_dir,\n",
    "        model_name, batch_size, num_train_epochs, gradient_accumulation_steps, lr, eval_steps):\n",
    "    ''' more general training arguments you likely want to keep fixed'''\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "\n",
    "        seed       = 42,\n",
    "        # use_cpu    = False, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "\n",
    "        output_dir = os.path.join(output_dir, model_name),\n",
    "\n",
    "        # NOTE: training params\n",
    "        learning_rate    = lr,\n",
    "        num_train_epochs = num_train_epochs,\n",
    "        # Use a smaller batch size to fit into GPU RAM. \n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size  = batch_size,\n",
    "        # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "        # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "        # NOTE: Evaluation params\n",
    "        # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "        evaluation_strategy = 'steps',\n",
    "        eval_steps = eval_steps,\n",
    "        save_steps = eval_steps,\n",
    "\n",
    "        logging_first_step=True,\n",
    "        logging_steps=100,\n",
    "        report_to  = 'wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model, \n",
    "        args = training_args, \n",
    "        train_dataset = train_dataset, \n",
    "        eval_dataset = eval_dataset,\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=isinstance(model, RobertaForMaskedLM)),\n",
    "    )\n",
    "\n",
    "    # print amount of training steps, and how often the model is evaluated\n",
    "    print(f'''\n",
    "    Retrieving Trainer for \\033[1m{model_name}\\033[0m\n",
    "        training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "        {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "        gives {len(train_dataset)//(batch_size * gradient_accumulation_steps)} training steps.\n",
    "        Evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "        ''')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Retrieving Trainer for \u001b[1mNEW_INFINI_GPT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 100 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 0 training steps.\n",
      "        Evaluating every 0 steps, 10 samples \n",
      "        \n",
      "\n",
      "    Retrieving Trainer for \u001b[1mBERT-9.0M-2L-4H-516C-1024I\u001b[0m\n",
      "        training for 1 epochs, 100 samples\n",
      "        16 batch size, 16 accumulation steps\n",
      "        gives 0 training steps.\n",
      "        Evaluating every 0 steps, 10 samples \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "out_dir = './results/new_infini/' \n",
    "\n",
    "trainer_gpt = get_trainer(gpt, tok_gpt, train_dataset, eval_dataset, out_dir, **params_gpt)\n",
    "trainer_rob = get_trainer(rob, tok_rob, train_dataset, eval_dataset, out_dir, **params_rob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train. \n",
    "\n",
    "This configuration takes ≤24hr to pre-train on my M1 Macbook Air with 16GB RAM. Python takes ≤4GB VRAM at a `batch_size=16` and ≤11GB at `batch_size=64`, though they take the same amount of time to train - likely because this processor is not designed to move that much data in and out of RAM constantly. And tbh, the GPU be lacking. If you decide to go the local-training-route, consider [chai](https://github.com/lvillani/chai) to keep your (Apple) laptop awake – there's probably a windows/linux equivalent too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(trainer: Trainer, name: str, out_dir: str): \n",
    "\n",
    "    # wandb.init(project='tiny-transformers', name=name, group='baseline', config=trainer.args)\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(out_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_train(trainer_rob, params_rob['model_name'], out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [01:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have labels\n",
      "we do this loss\n",
      "input shape torch.Size([10, 8, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m do_train(trainer_gpt, params_gpt[\u001b[39m'\u001b[39;49m\u001b[39mmodel_name\u001b[39;49m\u001b[39m'\u001b[39;49m], out_dir) \n",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_train\u001b[39m(trainer: Trainer, name: \u001b[39mstr\u001b[39m, out_dir: \u001b[39mstr\u001b[39m): \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# wandb.init(project='tiny-transformers', name=name, group='baseline', config=trainer.args)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     trainer\u001b[39m.\u001b[39msave_model(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(out_dir, name))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1864\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1865\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1866\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1867\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1868\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1869\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1870\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2205\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2208\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2209\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   2210\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTRLOSSSTEP\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m \u001b[39m# if self.control.should_evaluate:\u001b[39;00m\n\u001b[1;32m   2673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2675\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2677\u001b[0m     \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:3495\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3492\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3494\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3495\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3496\u001b[0m     eval_dataloader,\n\u001b[1;32m   3497\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3498\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3499\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3500\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3501\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3502\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3503\u001b[0m )\n\u001b[1;32m   3505\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:3678\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3675\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   3677\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3678\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   3679\u001b[0m main_input_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mmain_input_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3680\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:3866\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3864\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m   3865\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwe do this loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3866\u001b[0m     loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3867\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   3869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/trainer.py:3189\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3188\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3189\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   3190\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3191\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/tiny-transformers/code/lauri/modeling_gptneo.py:634\u001b[0m, in \u001b[0;36mInfiniAttentionGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    632\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 634\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    635\u001b[0m     input_ids,\n\u001b[1;32m    636\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    638\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    639\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    640\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    641\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    642\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    643\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    644\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    645\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    646\u001b[0m )\n\u001b[1;32m    647\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    648\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/tiny-transformers/code/lauri/modeling_gptneo.py:376\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    374\u001b[0m     past_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minput shape\u001b[39m\u001b[39m\"\u001b[39m, input_shape)\n\u001b[0;32m--> 376\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m     position_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(past_length, seq_length \u001b[39m+\u001b[39m past_length, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "do_train(trainer_gpt, params_gpt['model_name'], out_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right after training\n",
    "trained_model = trainer_gpt.model\n",
    "model_name = params_gpt['model_name']\n",
    "\n",
    "torch.save(trained_model.state_dict(), f'./results/new_infini/{model_name}/model_state.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before evaluation\n",
    "model_name = params_gpt['model_name']\n",
    "config_1 = GPTNeoConfig.from_pretrained(f'./results/new_infini/{model_name}')\n",
    "# config_1 = GPTNeoConfig.from_pretrained(f'./results/infini_baseline/fully_trained/')\n",
    "\n",
    "# config_infini = GPTNeoConfig.from_pretrained('./results/models_baseline/fully_trained')\n",
    "custom_model_1 = InfiniAttentionGPTNeoForCausalLM(config=config_1)\n",
    "trained_state_dict_1 = torch.load(f'./results/new_infini/{model_name}/model_state.pt', map_location=torch.device('cpu'))\n",
    "# trained_state_dict_1 = torch.load(f'./results/infini_baseline/fully_trained/model_state.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "custom_model_1.load_state_dict(trained_state_dict_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using your Pre-Trained Model \n",
    "Try out your own model on some prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a small function for pretty printing into paragraphs with line wrapping for readability\n",
    "import textwrap \n",
    "w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.4412]],\n",
      "\n",
      "         [[-0.5710]],\n",
      "\n",
      "         [[ 0.2974]],\n",
      "\n",
      "         [[-0.1022]]]])\n",
      "<class 'modeling_gptneo.InfiniAttentionGPTNeoForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = 'GPT-INFINI-9.0MINFINI-2L-4H-516C-1024I'\n",
    "vanilla = 'GPT-9.0M-2L-4H-516C-1024I'\n",
    "# model_name = 'BERT-9.0M-2L-4H-516C-1024I' # bert won't work for generation unless you fine-tune it for that task\n",
    "infin_name = 'INFINI_GPT-9.0M-2L-4H-516C-1024I'\n",
    "tokenizer = AutoTokenizer.from_pretrained('10k-tok')\n",
    "# tokenizer_old = AutoTokenizer.from_pretrained('10k-gpt-neo')\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# model = InfiniAttentionGPTNeoForCausalLM.from_pretrained(f'results/models_baseline/{infin_name}', use_safetensors=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(f'results/models_baseline/{infin_name}')\n",
    "state_dict = custom_model_1.state_dict()\n",
    "print(state_dict['transformer.h.0.attn.attention.betas'])\n",
    "# custom_model.load_state_dict(state_dict)\n",
    "print(type(custom_model_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "# import torch\n",
    "\n",
    "# # Path to your .safetensors file\n",
    "# safetensors_file_path = 'results/models_baseline/NEW_INFINI_GPT-9.0M-2L-4H-516C-1024I/model.safetensors'\n",
    "# safetensors_file_path = 'results/infini_baseline/fully_trained/model.safetensors'\n",
    "# # trained_model_safetensor_path = 'results/infini_baseline/fully_trained/checkpoint-8280/model.safetensors'\n",
    "\n",
    "\n",
    "# # Open the safetensors file\n",
    "# with safe_open(safetensors_file_path, framework=\"pt\") as f:\n",
    "#     # List all tensor names\n",
    "#     tensor_names = f.keys()\n",
    "#     print(\"Tensors in the safetensors file:\")\n",
    "#     for name in tensor_names:\n",
    "#         print(name)\n",
    "\n",
    "#     # Load a specific tensor\n",
    "#     tensor_name = tensor_names[0]  # Replace with the name of the tensor you want to load\n",
    "#     tensor = f.get_tensor(tensor_name)\n",
    "#     tensor_name2 = tensor_names[14]  # Replace with the name of the tensor you want to load\n",
    "#     tensor2 = f.get_tensor(tensor_name2)\n",
    "#     print(f\"Tensor {tensor_name}:\")\n",
    "#     print(f\"Tensor {tensor_name2}:\")\n",
    "\n",
    "#     print(tensor[0])\n",
    "#     print(tensor2[0])\n",
    "\n",
    "#     print(tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of tensor1:\n",
      "tensor([[[0.2826]],\n",
      "\n",
      "        [[0.9144]],\n",
      "\n",
      "        [[0.0287]],\n",
      "\n",
      "        [[0.7601]]])\n",
      "\n",
      "Sigmoid of tensor2:\n",
      "tensor([[[0.1094]],\n",
      "\n",
      "        [[0.6834]],\n",
      "\n",
      "        [[0.2717]],\n",
      "\n",
      "        [[0.4360]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the tensors\n",
    "tensor1 = torch.tensor([[[-0.9318]], [[ 2.3685]], [[-3.5218]], [[ 1.1535]]])\n",
    "tensor2 = torch.tensor([[[-2.0966]], [[ 0.7694]], [[-0.9860]], [[-0.2574]]])\n",
    "\n",
    "# Apply the sigmoid function\n",
    "sigmoid_tensor1 = torch.sigmoid(tensor1)\n",
    "sigmoid_tensor2 = torch.sigmoid(tensor2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Sigmoid of tensor1:\")\n",
    "print(sigmoid_tensor1)\n",
    "\n",
    "print(\"\\nSigmoid of tensor2:\")\n",
    "print(sigmoid_tensor2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "\n",
    "# # Define the tensors\n",
    "# tensor1 = torch.tensor([[[-0.9318]], [[ 2.3685]], [[-3.5218]], [[ 1.1535]]])\n",
    "# tensor2 = torch.tensor([[[-2.0966]], [[ 0.7694]], [[-0.9860]], [[-0.2574]]])\n",
    "\n",
    "# # Apply the sigmoid function\n",
    "# sigmoid_tensor1 = torch.sigmoid(tensor1)\n",
    "# sigmoid_tensor2 = torch.sigmoid(tensor2)\n",
    "\n",
    "# # Convert tensors to numpy arrays for plotting\n",
    "# sigmoid_tensor1_np = sigmoid_tensor1.numpy().flatten()\n",
    "# sigmoid_tensor2_np = sigmoid_tensor2.numpy().flatten()\n",
    "\n",
    "# # Combine the tensors into a matrix\n",
    "# data = [sigmoid_tensor1_np, sigmoid_tensor2_np]\n",
    "\n",
    "# # Create a heatmap\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# sns.heatmap(data, annot=True, cmap='Reds', cbar=False, square=True, xticklabels=False, yticklabels=['Attention Head 1', 'Attention Head 2'])\n",
    "# plt.xlabel('Values')\n",
    "# plt.ylabel('')\n",
    "# plt.title('Beta Values Heatmap')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference \n",
    "Let's generate a short story like the ones the model has been trained on! You'll notice that the prompt is surrounded by `<s>`, the begin-of-sequence (bos) token, and `</s>` end-of-sequence (eos) / separator (sep) token. This is from the BERT-style tokenisation, making it clear to the model where (one of several) input sequences ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3712 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n",
      "input shape torch.Size([1, 64])\n",
      "input shape (1, 64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3712) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print(len(input_ids[0]))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# segment_length = 64\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# input_segments = torch.tensor_split(input_ids[0], list(range(segment_length, input_ids.shape[1], segment_length)))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# for i in range(len(input_segments) - 1):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#     outputs = custom_model_1(input_ids=input_segments[i].unsqueeze(0))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m custom_model_1\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# input_ids=input_segments[-1].unsqueeze(0), # input to the model\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m4000\u001b[39;49m,                         \u001b[39m# maximum generation length\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,    \u001b[39m# early stopping when eos_token is output\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,                            \u001b[39m# number of beams to use in generation\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# print(output)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laurikeskull/Documents/Programming/tiny-transformers/code/lauri/infini_trainer.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/generation/utils.py:1579\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1563\u001b[0m         input_ids,\n\u001b[1;32m   1564\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1576\u001b[0m     )\n\u001b[1;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1578\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1579\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_greedy_search(\n\u001b[1;32m   1580\u001b[0m         input_ids,\n\u001b[1;32m   1581\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1582\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1583\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1584\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1585\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1586\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1587\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1588\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1589\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1590\u001b[0m     )\n\u001b[1;32m   1592\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1593\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/generation/utils.py:2498\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2497\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2499\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2500\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2501\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2502\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2503\u001b[0m )\n\u001b[1;32m   2505\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2506\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/tiny-transformers/code/lauri/modeling_gptneo.py:633\u001b[0m, in \u001b[0;36mInfiniAttentionGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    631\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 633\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    634\u001b[0m     input_ids,\n\u001b[1;32m    635\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    636\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    637\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    638\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    639\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    640\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    641\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    642\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    643\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    644\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    645\u001b[0m )\n\u001b[1;32m    646\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    647\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/tiny-transformers/code/lauri/modeling_gptneo.py:400\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    397\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask \u001b[39mif\u001b[39;00m (attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39min\u001b[39;00m attention_mask) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     \u001b[39m# 4d mask is passed through the layers\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m     attention_mask \u001b[39m=\u001b[39m _prepare_4d_causal_attention_mask(attention_mask, input_shape, inputs_embeds, past_length)\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     token_type_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwte(token_type_ids)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/modeling_attn_mask_utils.py:330\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39m# 4d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(attention_mask\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 330\u001b[0m     attention_mask \u001b[39m=\u001b[39m attn_mask_converter\u001b[39m.\u001b[39;49mto_4d(\n\u001b[1;32m    331\u001b[0m         attention_mask, input_shape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], key_value_length\u001b[39m=\u001b[39;49mkey_value_length, dtype\u001b[39m=\u001b[39;49minputs_embeds\u001b[39m.\u001b[39;49mdtype\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[39melif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(attention_mask\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    334\u001b[0m     expected_shape \u001b[39m=\u001b[39m (input_shape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, input_shape[\u001b[39m1\u001b[39m], key_value_length)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/transformers/modeling_attn_mask_utils.py:143\u001b[0m, in \u001b[0;36mAttentionMaskConverter.to_4d\u001b[0;34m(self, attention_mask_2d, query_length, dtype, key_value_length)\u001b[0m\n\u001b[1;32m    137\u001b[0m expanded_attn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_mask(attention_mask_2d, dtype, tgt_len\u001b[39m=\u001b[39minput_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(\n\u001b[1;32m    138\u001b[0m     attention_mask_2d\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m causal_4d_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     expanded_attn_mask \u001b[39m=\u001b[39m causal_4d_mask\u001b[39m.\u001b[39;49mmasked_fill(expanded_attn_mask\u001b[39m.\u001b[39;49mbool(), torch\u001b[39m.\u001b[39;49mfinfo(dtype)\u001b[39m.\u001b[39;49mmin)\n\u001b[1;32m    145\u001b[0m \u001b[39m# expanded_attn_mask + causal_4d_mask can cause some overflow\u001b[39;00m\n\u001b[1;32m    146\u001b[0m expanded_4d_mask \u001b[39m=\u001b[39m expanded_attn_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3712) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time, there was a little car named Beep. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly. He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.He then drove around quickly.'\n",
    "# prompt = 'Once upon a time, there was a little car named Beep. The little car had the name'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "# print(len(input_ids[0]))\n",
    "# segment_length = 64\n",
    "# input_segments = torch.tensor_split(input_ids[0], list(range(segment_length, input_ids.shape[1], segment_length)))\n",
    "\n",
    "# for i in range(len(input_segments) - 1):\n",
    "#     outputs = custom_model_1(input_ids=input_segments[i].unsqueeze(0))\n",
    "\n",
    "output = custom_model_1.generate(\n",
    "    # input_ids=input_segments[-1].unsqueeze(0), # input to the model\n",
    "    input_ids=input_ids,\n",
    "    max_length=4000,                         # maximum generation length\n",
    "    eos_token_id=tokenizer.eos_token_id,    # early stopping when eos_token is output\n",
    "    num_beams=1,                            # number of beams to use in generation\n",
    "    temperature=1,\n",
    ")\n",
    "# print(output)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "\n",
    "# textwrap with indentation on every new paragraph\n",
    "see(output_text)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds like me after a few beers too many, but at least the grammar is (mostly) correct. The model also learns some basic reasoning-like associations like being 'so high' allows you to see 'the whole world'. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
