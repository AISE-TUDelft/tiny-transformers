{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f19e7f320fc278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T14:34:17.536953Z",
     "start_time": "2024-06-06T14:34:17.408941Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install torch wandb transformers[torch] datasets tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T15:30:34.903310Z",
     "start_time": "2024-06-06T15:30:34.812124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.login(key=\"6f46f55bd51d76400f1e877ea7dfa75c5c7d05d6\")\n",
    "\n",
    "from transformers import GPT2TokenizerFast, GPTNeoForCausalLM, GPTNeoConfig, AutoTokenizer, AlbertTokenizer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import RobertaForCausalLM\n",
    "\n",
    "from tokenizers import Tokenizer, pre_tokenizers, decoders, AddedToken, normalizers, trainers\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "\n",
    "from tokenizers.processors import RobertaProcessing, TemplateProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa79b5e71accddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T14:34:19.498515Z",
     "start_time": "2024-06-06T14:34:19.074229Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_texts(dataset, split='train'):\n",
    "    for example in dataset[split]:\n",
    "        yield example['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffecfb80d9a1ed62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T14:34:31.894Z",
     "start_time": "2024-06-06T14:34:27.624173Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('deven367/babylm-100M', num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e17952c2c07e309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T20:43:11.070762Z",
     "start_time": "2024-06-01T20:40:03.807001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10176300/10176300 [02:58<00:00, 56869.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m iterator \u001b[38;5;241m=\u001b[39m get_texts(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m iterator \u001b[38;5;241m=\u001b[39m tqdm(iterator, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Manually add the special tokens with detailed configurations\u001b[39;00m\n\u001b[1;32m     42\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens([t\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m special_tokens])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(\n",
    "    dropout=None,\n",
    "    unk_token=\"<unk>\",\n",
    "    continuing_subword_prefix=\"Ġ\",\n",
    "    end_of_word_suffix=\"\",\n",
    "    fuse_unk=False,\n",
    "    byte_fallback=False,\n",
    "    ignore_merges=False\n",
    "))\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True)\n",
    "tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)\n",
    "\n",
    "# Configure post-processor\n",
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    sep=(\"</s>\", 2),\n",
    "    cls=(\"<s>\", 1),\n",
    "    trim_offsets=True,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\n",
    "    AddedToken(\"<pad>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"</s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<unk>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "]\n",
    "\n",
    "# Create the trainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=3000, \n",
    "    special_tokens=[str(t.content) for t in special_tokens]\n",
    ")\n",
    "\n",
    "iterator = get_texts(dataset, 'train')\n",
    "iterator = tqdm(iterator, total=len(dataset['train']))\n",
    "\n",
    "tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "\n",
    "# Manually add the special tokens with detailed configurations\n",
    "tokenizer.add_special_tokens([t.content for t in special_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d87e04f40c745cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T21:01:41.230260Z",
     "start_time": "2024-06-01T20:58:19.513467Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10176300/10176300 [03:10<00:00, 53415.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordPiece Tokenizer Creator\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(\n",
    "    unk_token=\"<unk>\"\n",
    "))\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True)\n",
    "tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)\n",
    "\n",
    "# Configure post-processor\n",
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    sep=(\"</s>\", 2),\n",
    "    cls=(\"<s>\", 1),\n",
    "    trim_offsets=True,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\n",
    "    AddedToken(\"<pad>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"</s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<unk>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "]\n",
    "\n",
    "# Create the trainer\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=20000, \n",
    "    special_tokens=[str(t.content) for t in special_tokens]\n",
    ")\n",
    "\n",
    "iterator = get_texts(dataset, 'train')\n",
    "iterator = tqdm(iterator, total=len(dataset['train']))\n",
    "\n",
    "tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "\n",
    "# Manually add the special tokens with detailed configurations\n",
    "tokenizer.add_special_tokens([t.content for t in special_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e013437f192302ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T15:05:34.006618Z",
     "start_time": "2024-06-06T15:05:33.951364Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_alphabet = list(\"\"\"!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥¦§¨©ª«¬®¯°±²³´µ¶·¸¹º»¼½¾¿ÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞàáâãäåæçèéêëìíîïðĉČĠġĢģĤĥĦħĨĩĪīĬĭĮįİıĲĳĴĵĶķĸĹĺĻļĽľĿŀŁłŃ\"\"\")\n",
    "\n",
    "def get_texts(dataset, split='train'):\n",
    "    for example in dataset[split]:\n",
    "        yield example['text']\n",
    "\n",
    "# Define a function to filter out non-English text\n",
    "def is_english(text):\n",
    "    return all(c in initial_alphabet or c.isspace() for c in text)\n",
    "\n",
    "# Create a generator with a progress bar and filtering\n",
    "def line_generator(dataset, split='train'):\n",
    "    texts = get_texts(dataset, split)\n",
    "    for text in tqdm(texts, desc=\"Training SentencePiece Tokenizer\"):\n",
    "        filtered_text = ''.join([c for c in text if c in initial_alphabet or c.isspace()])\n",
    "        if filtered_text:\n",
    "            yield filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79aee471193d8aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T15:17:24.628466Z",
     "start_time": "2024-06-06T15:05:46.042270Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SentencePiece Tokenizer: 10176300it [10:48, 15696.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokenizer training complete.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceUnigramTokenizer()\n",
    "\n",
    "# Define the normalizer and pre-tokenizer\n",
    "# bert_normalizer = BertNormalizer(handle_chinese_chars=False)\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Define the trainer for the Unigram model\n",
    "special_tokens = [\n",
    "    AddedToken(\"<pad>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"</s>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "    AddedToken(\"<unk>\", single_word=False, lstrip=False, rstrip=False, normalized=True, special=True),\n",
    "]\n",
    "\n",
    "# iterator = get_texts(dataset, 'train')\n",
    "# iterator = tqdm(iterator, total=len(dataset['train']))\n",
    "# Train the tokenizer using the generator\n",
    "tokenizer.train_from_iterator(line_generator(dataset, 'train'), vocab_size=6000, special_tokens=special_tokens, unk_token=\"<unk>\", initial_alphabet=initial_alphabet)\n",
    "\n",
    "# Save the trained tokenizer model\n",
    "tokenizer.save(\"sentencepiece_unigram_tokenizer\")\n",
    "\n",
    "print(\"Tokenizer training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3790ee31d7add7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T19:31:27.698195Z",
     "start_time": "2024-06-06T19:29:49.832366Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m cleaned_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_training_data.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the SentencePiece model with a restricted character set\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentencepiece_unigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust the vocabulary size as needed\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcharacter_coverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_sentence_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_input_sentence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# user_defined_symbols=list(initial_alphabet),\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Padding ID (default is 0)\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unknown token ID (default is 1)\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Beginning of sentence ID (default is 2)\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# End of sentence ID (default is 3)\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# user_defined_symbols=\"<mask>\"\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tokenizer/lib/python3.10/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tokenizer/lib/python3.10/site-packages/sentencepiece/__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/tokenizer/lib/python3.10/site-packages/sentencepiece/__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define the initial alphabet as a string\n",
    "initial_alphabet = set(\"\".join([\"!\\\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥¦§¨©ª«¬®¯°±²³´µ¶·¸¹º»¼½¾¿ÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞàáâãäåæçèéêëìíîïðĉČĠġĢģĤĥĦħĨĩĪīĬĭĮįİıĲĳĴĵĶķĸĹĺĻļĽľĿŀŁłŃ \"]))\n",
    "\n",
    "\n",
    "cleaned_file = 'cleaned_training_data.txt'\n",
    "\n",
    "# Train the SentencePiece model with a restricted character set\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=cleaned_file,\n",
    "    model_prefix='sentencepiece_unigram',\n",
    "    vocab_size=2999,  # Adjust the vocabulary size as needed\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0,\n",
    "    input_sentence_size=100000,\n",
    "    shuffle_input_sentence=True,\n",
    "    # user_defined_symbols=list(initial_alphabet),\n",
    "    pad_id=0,  # Padding ID (default is 0)\n",
    "    unk_id=1,  # Unknown token ID (default is 1)\n",
    "    bos_id=2,  # Beginning of sentence ID (default is 2)\n",
    "    eos_id=3,  # End of sentence ID (default is 3)\n",
    "    # user_defined_symbols=\"<mask>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d55cae5294a9725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T19:32:05.906530Z",
     "start_time": "2024-06-06T19:32:05.880858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertTokenizer(name_or_path='', vocab_size=5999, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5999: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Encoded ids: [2, 41, 20, 9, 1223, 3386, 10, 1275, 7, 10, 2769, 1495, 78, 4, 3]\n",
      "Decoded text: <s> this is a test sentence to check the tokenizer.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, BertTokenizer, T5Tokenizer, PreTrainedTokenizerFast\n",
    "import tokenizers\n",
    "\n",
    "model_path = 'tokenizers/6k-sp/sp_unigram.model'\n",
    "vocab_path = 'tokenizers/6k-sp/sp_unigram.vocab'\n",
    "# config_path = 'tokenizers/sentence-piece/tokenizer_config.json'\n",
    "# special_tokens_map_path = 'tokenizers/sentence-piece/special_tokens_map.json'\n",
    "\n",
    "# Create a custom tokenizer\n",
    "# tokenizer = BertTokenizer(\n",
    "#     vocab_file=vocab_path,    \n",
    "#     sp_model_file=model_path\n",
    "# )\n",
    "\n",
    "# tokenizer = tokenizers.SentencePieceUnigramTokenizer().from_spm(\"tokenizers/sentence-piece/sentencepiece_unigram.model\")\n",
    "# print(type(tokenizer.model))\n",
    "\n",
    "# Load the T5Tokenizer with the SentencePiece model\n",
    "# tokenizer = T5Tokenizer(vocab_file=model_path)\n",
    "\n",
    "\n",
    "# Load the AlbertTokenizer with the SentencePiece model\n",
    "tokenizer = AlbertTokenizer(\n",
    "    sp_model_file=model_path,\n",
    "    mask_token=\"<mask>\",\n",
    "    cls_token=\"<s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    vocab_file=model_path)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "# Sample text to test the tokenizer\n",
    "sample_text = \"This is a test sentence to check the tokenizer.\"\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(f\"Encoded ids: {encoded}\")\n",
    "\n",
    "# Decode the ids back to text\n",
    "decoded_text = tokenizer.decode(encoded)\n",
    "print(f\"Decoded text: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ae19bc90696dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T19:32:19.931028Z",
     "start_time": "2024-06-06T19:32:19.921943Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizers/3k-sp/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./tokenizers/3k-sp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb5416c3b0a2239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:25:01.528191Z",
     "start_time": "2024-06-06T17:25:01.462340Z"
    }
   },
   "outputs": [],
   "source": [
    "test_tok = AlbertTokenizer.from_pretrained(\"./tokenizers/3k-sp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3312d1c453192e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-06T17:21:56.022488Z",
     "start_time": "2024-06-06T17:21:55.982807Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AlbertTokenizer' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizers/sentence-piece/6k-sp/tokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tokenizers/sentence-piece/6k-sp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AlbertTokenizer' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "tokenizer.save(\"./tokenizers/sentence-piece/6k-sp/tokenizer.json\")\n",
    "tokenizer.model.save(\"./tokenizers/sentence-piece/6k-sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20d30e6c9e8509f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T22:27:00.018143Z",
     "start_time": "2024-06-04T22:26:08.450267Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         vocab_dict[token] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the dictionary to a JSON file\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m(vocab_json_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(vocab_dict, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         vocab_dict[token] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the dictionary to a JSON file\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m(vocab_json_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(vocab_dict, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/snap/pycharm-professional/391/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n",
      "File \u001b[0;32m/snap/pycharm-professional/391/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/snap/pycharm-professional/391/plugins/python/helpers/pydev/pydevd.py:1187\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_id)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/snap/pycharm-professional/391/plugins/python/helpers/pydev/pydevd.py:1202\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 1202\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TOkenizer changes\n",
    "\n",
    "import json\n",
    "\n",
    "# Path to the input vocabulary text file\n",
    "vocab_text_file = './tokenizers/20k-wp/vocab.txt'\n",
    "\n",
    "# Path to the output JSON file\n",
    "vocab_json_file = './tokenizers/20k-wp/vocab.json'\n",
    "\n",
    "# Read the vocabulary text file and create a dictionary\n",
    "vocab_dict = {}\n",
    "with open(vocab_text_file, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        token = line.strip()  # Remove any leading/trailing whitespace\n",
    "        vocab_dict[token] = idx\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open(vocab_json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1314ac95718f5b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T19:46:56.496291Z",
     "start_time": "2024-06-01T19:46:56.462077Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\n",
    "        'tokenizers/3k-bpe/',                                         # our custom tokenizer\n",
    "        model_max_length=512    # sequence length (context window)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
