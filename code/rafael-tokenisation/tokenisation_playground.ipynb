{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T09:18:25.966561Z",
     "start_time": "2024-05-14T09:18:20.790165Z"
    }
   },
   "source": [
    "import wandb; wandb.login()\n",
    "from transformers import GPT2TokenizerFast, GPTNeoForCausalLM, GPTNeoConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import RobertaForCausalLM\n",
    "\n",
    "config = GPTNeoConfig(\n",
    "\n",
    "    # number of tokens in the vocabulary \n",
    "    vocab_size = 10_000, \n",
    "    # embedding size (vector length) of each token \n",
    "    hidden_size=512, \n",
    "    # we thus have an embedding block of 512 x 10'000 parameters\n",
    "\n",
    "    # maximum sequence length, though inputs longer than `hidden_size` will be iteratively processed\n",
    "    max_position_embeddings = 512, \n",
    "\n",
    "    # number of transformer blocks. div by 2 for attention_types\n",
    "    num_layers=2, \n",
    "    # for global and local attention (GPT-Neo-specific)\n",
    "    attention_types=[[[\"global\", \"local\"], 1]], \n",
    "\n",
    "    num_heads=4,     # attention heads\n",
    "    window_size=256, # for local attention (GPT-Neo-specific)\n",
    "\n",
    "    intermediate_size=1024, # size of 'up-projection' layer in FFN\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mr-bragamedeirosmotaborges\u001B[0m (\u001B[33mtiny-transformers\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:43:15.077589Z",
     "start_time": "2024-05-14T09:43:14.805825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('EleutherAI/gpt-neo-125m', model_max_length=config.hidden_size)\n",
    "\n",
    "# in theory, window_size x num_layers is possible. \n",
    "# But, this project is not about the efficacy of sliding window attention (though it could be!)\n",
    "assert tokenizer.model_max_length == 512 \n",
    "# assert tokenizer.vocab_size == config.vocab_size\n",
    "\n",
    "# printing this because of a bug in tokenizers (should be fixed now) https://github.com/huggingface/transformers/issues/26500\n",
    "print(f'padding token is {tokenizer.pad_token}')\n",
    "# HF wasn't saving this nor the tokenizer's pad_token\n",
    "config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "8e76b0e206aada07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding token is None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:57:12.342997Z",
     "start_time": "2024-05-14T09:48:23.641003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json \n",
    "tokenizer_state = json.loads(tokenizer.backend_tokenizer.model.__getstate__())\n",
    "\n",
    "n_words = len(tokenizer_state['vocab'])\n",
    "print(f'vocabulary: {n_words}') # would be lovely if this was 10'000\n",
    "\n",
    "# index n_words - 1 contains the special eos token `<|endoftext|>`\n",
    "new_vocab = {k: v for k, v in tokenizer_state['vocab'].items() if v < 10_000-1 or v == n_words-1}\n",
    "tokenizer_state['vocab'] = new_vocab\n",
    "\n",
    "# you can see that most common tokens are listed first (individual characters, pairs of chars, triples, etc.)\n",
    "print(f'new vocab : {len(new_vocab)}, {list(new_vocab.keys())[:3]}, {list(new_vocab.keys())[-3:]}')\n",
    "\n",
    "\n",
    "# Updating the tokenizer with new vocab: of course this doesn't work the first try, for some reason. \n",
    "from tokenizers import models\n",
    "model_class = getattr(models, tokenizer_state.pop('type'))\n",
    "\n",
    "# 'str' object cannot be converted to 'PyTuple'\n",
    "# tokenizer.backend_tokenizer.model = model_class(**tokenizer_state) \n",
    "\n",
    "# Let's manually create tuple objects, maybe Rust's type safety is keeping us safe. \n",
    "new_merges = [tuple(m.split()) for m in tokenizer_state['merges']]\n",
    "print(f'new merges: {len(new_merges)}, {new_merges[:3]}, {new_merges[-3:]}') # Ġ means space ' ' \n",
    "tokenizer_state['merges'] = new_merges\n",
    "\n",
    "# tokenizer.backend_tokenizer.model = model_class(**tokenizer_state) # Token `ordon` out of vocabulary"
   ],
   "id": "bed3af369c9f020c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 50257\n",
      "new vocab : 10000, ['!', '\"', '#'], ['oret', 'ths', '<|endoftext|>']\n",
      "new merges: 50000, [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e')], [('ĠColl', 'ider'), ('Ġinform', 'ants'), ('Ġg', 'azed')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m new_merges \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(m\u001B[38;5;241m.\u001B[39msplit()) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m tokenizer_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerges\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew merges: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(new_merges)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_merges[:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_merges[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m:]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# Ġ means space ' ' \u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m tokenizer_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerges\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mnew_merges\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# tokenizer.backend_tokenizer.model = model_class(**tokenizer_state) # Token `ordon` out of vocabulary\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[8], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m new_merges \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(m\u001B[38;5;241m.\u001B[39msplit()) \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m tokenizer_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerges\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew merges: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(new_merges)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_merges[:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_merges[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m:]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# Ġ means space ' ' \u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m tokenizer_state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerges\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mnew_merges\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# tokenizer.backend_tokenizer.model = model_class(**tokenizer_state) # Token `ordon` out of vocabulary\u001B[39;00m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:888\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    885\u001B[0m             stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[0;32m--> 888\u001B[0m     stopped_on_plugin \u001B[38;5;241m=\u001B[39m \u001B[43mplugin_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[1;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[0;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/pydevd.py:1185\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1182\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1184\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1185\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/pydevd.py:1200\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1197\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1199\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1200\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:04:03.254398Z",
     "start_time": "2024-05-14T12:04:03.226739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_merges = new_merges[:3000-257] # 256 says: \"Token `ordon` out of vocabulary\"\n",
    "tokenizer_state['merges'] = new_merges\n",
    "print(f'new merges: {len(new_merges)}, {new_merges[:3]}, {new_merges[-3:]}') # Ġ means space ' '\n",
    "\n",
    "\n",
    "tokenizer.backend_tokenizer.model = model_class(**tokenizer_state)\n",
    "print(f'our tokenizer now has the {len(tokenizer)} most frequent tokens (from whatever dataset it was trained on)')\n"
   ],
   "id": "91ecf53fae04ef68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merges: 2743, [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e')], [('6', '5'), ('Ġb', 'illion'), ('0', '7')]\n",
      "our tokenizer now has the 10000 most frequent tokens (from whatever dataset it was trained on)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:28:22.651769Z",
     "start_time": "2024-05-14T12:08:35.826706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenizer.save_pretrained('10k-tokenizer')\n",
    "print(tokenizer.decode(tokenizer.encode('hello worldaisudhgiashg asdugh')))\n",
    "tokenizer.vocab_size\n",
    "\n",
    "# NOTE: Saving hangs indefinitely, I don't know why and it requires me to look at the rust impl.\n",
    "# instead, I just ended up modifying the merges.txt and vocab.json files manually, following\n",
    "# exactly what I did above. \n",
    "\n",
    "tokenizer.name_or_path = '10k-tokenizer'\n",
    "# tokenizer.save_pretrained('10k-tokenizer') \n",
    "# tokenizer.save_vocabulary('.', '10k-tokenizer.json')\n"
   ],
   "id": "1c9fca3d6a0af31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello worldaisudhgiashg asdugh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# tokenizer.save_pretrained('10k-tokenizer')\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(tokenizer\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhello worldaisudhgiashg asdugh\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mvocab_size\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# NOTE: Saving hangs indefinitely, I don't know why and it requires me to look at the rust impl.\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# instead, I just ended up modifying the merges.txt and vocab.json files manually, following\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# exactly what I did above. \u001B[39;00m\n\u001B[1;32m      9\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mname_or_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m10k-tokenizer\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "Cell \u001B[0;32mIn[19], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# tokenizer.save_pretrained('10k-tokenizer')\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(tokenizer\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhello worldaisudhgiashg asdugh\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mvocab_size\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# NOTE: Saving hangs indefinitely, I don't know why and it requires me to look at the rust impl.\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# instead, I just ended up modifying the merges.txt and vocab.json files manually, following\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# exactly what I did above. \u001B[39;00m\n\u001B[1;32m      9\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mname_or_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m10k-tokenizer\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/pydevd.py:1185\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1182\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1184\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1185\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/387/plugins/python/helpers/pydev/pydevd.py:1200\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1197\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1199\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1200\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T12:04:15.428840Z",
     "start_time": "2024-05-14T12:04:15.412840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_merges = new_merges[:3000-257] # 256 says: \"Token `ordon` out of vocabulary\"\n",
    "tokenizer_state['merges'] = new_merges\n",
    "merge_file = open(\"3k-tok-bpe/merges.txt\", \"w\")\n",
    "merge_file.writelines([merge[0] + \" \" + merge[1] + \"\\n\" for merge in new_merges])\n",
    "merge_file.close()\n"
   ],
   "id": "58b866fcc8e3beaf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:06:54.482079Z",
     "start_time": "2024-05-14T13:06:54.468990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_merges = new_merges[:3000-257]\n",
    "tokenizer_state['merges'] = new_merges\n",
    "tokenizer.backend_tokenizer.model = model_class(**tokenizer_state)\n",
    "new_vocab = {k: v for k, v in tokenizer_state['vocab'].items() if v < 3_000-1 or v == n_words-1}\n",
    "new_vocab[\"!\"] = 2995\n",
    "new_vocab['\"'] = 2996\n",
    "new_vocab[\"#\"] = 2997\n",
    "new_vocab[\"$\"] = 2998\n",
    "new_vocab.pop(\"Ġevents\")\n",
    "new_vocab.pop(\"65\")\n",
    "new_vocab.pop(\"Ġbillion\")\n",
    "new_vocab.pop(\"07\")\n",
    "new_vocab[\"<pad>\"] = 0\n",
    "new_vocab[\"<s>\"] = 1\n",
    "new_vocab[\"</s>\"] = 2\n",
    "new_vocab[\"<unk>\"] = 3\n",
    "new_vocab[\"<mask>\"] = 2999\n",
    "new_vocab.pop(\"<|endoftext|>\")\n",
    "new_vocab = {k: v for k, v in sorted(new_vocab.items(), key=lambda item: item[1])}\n",
    "\n",
    "merge_file = open(\"3k-tok-bpe/vocab.json\", \"w\")\n",
    "merge_file.write(str(new_vocab))\n",
    "merge_file.close()"
   ],
   "id": "be26bc0b323e3262",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e7a591ef3dcbbe1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
