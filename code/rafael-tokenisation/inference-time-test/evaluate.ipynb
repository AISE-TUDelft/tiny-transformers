{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/babylm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, json\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = {\n",
    "    \"glue\": [\"cola\", \"sst2\", \"mrpc\", \"qqp\", \"mnli\", \"mnli-mm\", \"qnli\", \"rte\",\n",
    "             \"boolq\", \"multirc\", \"wsc\"],\n",
    "    \"blimp\": [\"anaphor_agreement\", \"argument_structure\", \"binding\", \"control_raising\",\n",
    "              \"determiner_noun_agreement\", \"ellipsis\", \"filler_gap\", \"irregular_forms\",\n",
    "              \"island_effects\", \"npi_licensing\", \"quantifiers\", \"subject_verb_agreement\"],\n",
    "    \"supplement\": [\"hypernym\", \"qa_congruence_easy\", \"qa_congruence_tricky\",\n",
    "                   \"subject_aux_inversion\", \"turn_taking\"],\n",
    "    \"msgs\": [\"main_verb_control\", \"control_raising_control\", \"syntactic_category_control\",\n",
    "             \"relative_position_control\", \"lexical_content_the_control\",\n",
    "             \"main_verb_lexical_content_the\", \"main_verb_relative_token_position\",\n",
    "             \"control_raising_lexical_content_the\", \"control_raising_relative_token_position\",\n",
    "             \"syntactic_category_lexical_content_the\", \"syntactic_category_relative_position\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, tokenizer_path, cuda_index=0): \n",
    "    ''' run babylm pipeline, log training to a file '''\n",
    "\n",
    "    with open(os.path.join(model_path, 'eval.log'), 'wb') as f:\n",
    "\n",
    "        cuda_index = cuda_index % 2\n",
    "        process = subprocess.Popen(f'CUDA_VISIBLE_DEVICES={cuda_index} ./evaluate.sh {model_path, tokenizer_path}', \n",
    "                stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "        for c in iter(lambda: process.stdout.read(1), b''):\n",
    "            sys.stdout.buffer.write(c)\n",
    "            f.write(c)\n",
    "\n",
    "        process.wait() # wait until the entire eval is done\n",
    "\n",
    "def glue_metric(task: str, results: dict[str, float]):\n",
    "    ''' returns the appropriate glue score for aggregating results '''\n",
    "\n",
    "    if task in ['sst2', 'mnli', 'mnli-mm', 'qnli', 'rte', 'boolq', 'wsc']:\n",
    "        return results['eval_accuracy']\n",
    "    elif task in ['cola']: return results['eval_mcc']\n",
    "    elif task in ['mrpc', 'qqp']: \n",
    "        return (results['eval_accuracy'] + results['eval_f1']) / 2 \n",
    "\n",
    "    elif task in ['multirc']:\n",
    "        # TODO: THIS SHOULD NOT BE MCC, BUT EXACT MATCH; SEE NOTES\n",
    "        return (results['eval_f1'] + results['eval_mcc']) / 2\n",
    "\n",
    "def eval_and_aggregate(model_dir, tokenizer_path, model_name, no_train=False, index=0) -> dict:\n",
    "    ''' run evaluation, find all scores in the model dir, and aggregate them according \n",
    "        to how the BLiMP/GLUE/Super(GLUE)/MSGS papers describe. \n",
    "        Then, return all (relevant) scores as a big dic'''\n",
    "\n",
    "    model_path = model_dir\n",
    "\n",
    "    # if the model_path contains finetune and zerosho, set no_train to true \n",
    "    if all(os.path.exists(os.path.join(model_path, x)) for x in ['finetune', 'zeroshot']):\n",
    "        print(f'\\033[1m{model_name} has already been trained; skipping training\\033[0m')\n",
    "        no_train = True\n",
    "\n",
    "    # Can be useful if you've already fine-tuned some models and don't want to do that again\n",
    "    if not no_train: \n",
    "        # if config.json is not in model_path, break from this function \n",
    "        if not os.path.exists(os.path.join(model_path, 'config.json')):\n",
    "            raise FileNotFoundError(f'config.json not found in {model_path}')\n",
    "\n",
    "        evaluate(model_path, tokenizer_path, cuda_index=index)\n",
    "\n",
    "    # assume you have the scores computed in the model's directory\n",
    "    blimp, supplement, glue, msgs = get_scores(model_path)\n",
    "\n",
    "    blimp_sub_avg = sum(task['eval_accuracy'] for task in blimp.values()) / len(blimp)\n",
    "    supplement_avg = sum(task['eval_accuracy'] for task in supplement.values()) / len(supplement)\n",
    "    blimp_avg = sum(task['eval_accuracy'] for task in [*blimp.values(), *supplement.values()]) / (len(blimp) + len(supplement))\n",
    "\n",
    "    glue_metrics = {task: glue_metric(task, results) for task, results in glue.items()}\n",
    "    glue_avg = sum(glue_metrics.values()) / len(glue_metrics)\n",
    "\n",
    "    print(f'''\n",
    "        \\033[1mMULTIRC NOT IMPLEMENTED PROPERLY; SEE NOTES\\033[0m\n",
    "        \\033[1mMSGS FINETUNED & INFERENCED, BUT NOT COMPUTED CORRECTLY AND OMMITTED\\033[0m\n",
    "\n",
    "        \\033[1mFinal scores for {model_name} \\033[0m\n",
    "        BLiMP: {blimp_avg*100:.1f}%  \\t ({blimp_sub_avg:.1f} base, {supplement_avg:.1f} supplement)\n",
    "        GLUE : {glue_avg*100:.1f}    \\t (multiplied by 100)\n",
    "        ''')\n",
    "\n",
    "    return {\n",
    "        # Aggregated (mostly averaged) score\n",
    "        'blimp_avg': blimp_avg,\n",
    "        'glue_avg': glue_avg,\n",
    "\n",
    "        # Score per component; not sure if it's normal to report these separate or whether it\n",
    "        # was just a thing for the BabyLM challenge.\n",
    "        'base_avg': blimp_sub_avg,\n",
    "        'supp_avg': supplement_avg,\n",
    "\n",
    "        # Individual task scores, combined using the correct metrics. \n",
    "        # NOTE: except for MultiRC, for which we need to compute EM (simple in practice, im out of time tho)\n",
    "        **blimp, \n",
    "        **supplement,\n",
    "        **glue_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_scores(model_path) -> tuple[dict, dict, dict[str, dict], dict[str, dict]]:\n",
    "    ''' get the scores from the model's directory and create dictionaries \n",
    "        for each benchmark in the evaluation '''\n",
    "\n",
    "    # get ZEROSHOT scores \n",
    "    blimp, supplement = {}, {} \n",
    "    for task in os.listdir(os.path.join(model_path, 'zeroshot')):\n",
    "        with open(os.path.join(model_path, 'zeroshot', task, 'eval_results.json'), 'r') as f:\n",
    "\n",
    "            score = json.load(f)\n",
    "            if task in TASKS['blimp']: blimp[task] = score\n",
    "            elif task in TASKS['supplement']: supplement[task] = score\n",
    "            else: raise ValueError(f\"Invalid task: {task}!\")\n",
    "\n",
    "    # get FINETUNED scores \n",
    "    glue, msgs = {}, {} \n",
    "    for task in os.listdir(os.path.join(model_path, 'finetune')):\n",
    "        with open(os.path.join(model_path, 'finetune', task, 'all_results.json'), 'r') as f:\n",
    "\n",
    "            score = json.load(f)\n",
    "            if task in TASKS['glue']: glue[task] = score\n",
    "            elif task in TASKS['msgs']: msgs[task] = score\n",
    "            else: raise ValueError(f\"Invalid task: {task}!\")\n",
    "    \n",
    "    # print these babies\n",
    "    for name, benchmark in [('BLiMP', blimp), ('Supp.', supplement), ('GLUE', glue), ('MSGS', msgs)]: \n",
    "\n",
    "        print(f'\\n\\033[1m{name:>50s}  \\tAcc.\\t F1 \\t MCC\\033[0m')\n",
    "        for task, score in sorted(benchmark.items()):\n",
    "\n",
    "            acc = f'{score[\"eval_accuracy\"]*100:.2f}%' if 'eval_accuracy' in score else '-'\n",
    "            f1 = f'{score[\"eval_f1\"]:.2f}' if 'eval_f1' in score else '-'\n",
    "            mcc = f'{score[\"eval_mcc\"]:.2f}' if 'eval_mcc' in score else '-'\n",
    "\n",
    "            print(f'{task:>50s}: \\t{acc}\\t {f1}\\t {mcc}')\n",
    "\n",
    "    return blimp, supplement, glue, msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ['CONDA_DEFAULT_ENV'] == 'babylm':\n",
    "    print('\\033[1m WARNING: You are not in an environment named babylm \\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: Syntax error: \"(\" unexpected\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/GPT-test-tokenizers/10k-bpe-babylm-9.0M-2L-4H-516C-1024I/zeroshot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43meval_and_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/GPT-test-tokenizers/10k-bpe-babylm-9.0M-2L-4H-516C-1024I\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../tokenizers/10k-bpe-babylm/*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGPT-10k-BABYLM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 50\u001b[0m, in \u001b[0;36meval_and_aggregate\u001b[0;34m(model_dir, tokenizer_path, model_name, no_train, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     evaluate(model_path, tokenizer_path, cuda_index\u001b[38;5;241m=\u001b[39mindex)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# assume you have the scores computed in the model's directory\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m blimp, supplement, glue, msgs \u001b[38;5;241m=\u001b[39m \u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m blimp_sub_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m blimp\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(blimp)\n\u001b[1;32m     53\u001b[0m supplement_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m supplement\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(supplement)\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mget_scores\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# get ZEROSHOT scores \u001b[39;00m\n\u001b[1;32m     92\u001b[0m blimp, supplement \u001b[38;5;241m=\u001b[39m {}, {} \n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzeroshot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeroshot\u001b[39m\u001b[38;5;124m'\u001b[39m, task, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     96\u001b[0m         score \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/GPT-test-tokenizers/10k-bpe-babylm-9.0M-2L-4H-516C-1024I/zeroshot'"
     ]
    }
   ],
   "source": [
    "results = eval_and_aggregate(\"../models/GPT-test-tokenizers/10k-bpe-babylm-9.0M-2L-4H-516C-1024I\", \"../tokenizers/10k-bpe-babylm/*\", \"GPT-10k-BABYLM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
