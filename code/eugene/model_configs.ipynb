{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b97ac3a-778f-492a-a283-0305cddd2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sparse_ffn.sparsity_types import SparsityType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10c586-1622-4581-b48b-f5eeac54dd9a",
   "metadata": {},
   "source": [
    "# Baseline GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d7a900-2b66-4005-beda-c6de696edba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "\n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   # number of tokens in the vocabulary \n",
    "    hidden_size             = 256,      # embedding size (vector length) of each token \n",
    "    max_position_embeddings = 512,      # maximum sequence length (context window)\n",
    "\n",
    "    # BLOCKS (ATTN & FFN)\n",
    "    num_layers          = 2,                    # number of transformer blocks\n",
    "    attention_types     = [[[\"global\", \"local\"], 1]], # (GPT-Neo-specific) global and local attention \n",
    "    num_heads           = 4,                    # attention heads\n",
    "    window_size         = 256,                  # (GPT-Neo-specific) for local attention \n",
    "    intermediate_size   = 256 * 16,                 # size of 'up-projection' layer in FFN\n",
    "\n",
    "    pad_token_id = 0,           # need to specify this for tokenizer interop between models\n",
    ")\n",
    "\n",
    "with open(\"model_configs/gpt_baseline.json\", \"w\") as outfile: \n",
    "    json.dump(config, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ae0b1-10c2-4e57-9611-4bc48d62c204",
   "metadata": {},
   "source": [
    "# Baseline Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612f132a-b0db-4845-a967-7c737a6f970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = dict(\n",
    "    \n",
    "    # EMBEDDING PARAMETERS\n",
    "    vocab_size              = 10_000,   \n",
    "    hidden_size             = 256,      \n",
    "    # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "    max_position_embeddings = 512 + 1,\n",
    "\n",
    "    # BLOCKS (of course naming is different in roberta :) )\n",
    "    num_hidden_layers = 2,\n",
    "    num_attention_heads = 4,\n",
    "    intermediate_size=256 * 16,                     \n",
    "\n",
    "    pad_token_id = 0,\n",
    ")\n",
    "\n",
    "with open(\"model_configs/roberta_baseline.json\", \"w\") as outfile: \n",
    "    json.dump(config, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039d440-1f31-4eaa-8075-4107fd8ad716",
   "metadata": {},
   "source": [
    "# MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a641ef8-c568-4097-894d-b58280752ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate layer size is 1023\n",
      "low 3\n",
      "medium 2\n",
      "high 1\n"
     ]
    }
   ],
   "source": [
    "dim_in = 256\n",
    "intermediate_factor = 16 # for baseline ffn, intermediate size intermediate factor * input dim\n",
    "sparsity_type=SparsityType.MOE\n",
    "num_experts = 4\n",
    "topks= [('low', 3), ('medium',2), ('high',1)]\n",
    "intermediate_size = round( (intermediate_factor * dim_in - num_experts) / num_experts )\n",
    "print(\"intermediate layer size is\", intermediate_size)\n",
    "\n",
    "for pair in topks:\n",
    "    st, topk = pair\n",
    "    print(st, topk)\n",
    "    config_gpt = dict(\n",
    "    \n",
    "        # number of tokens in the vocabulary \n",
    "        vocab_size = 10_000, \n",
    "        # embedding size (vector length) of each token \n",
    "        hidden_size=dim_in, \n",
    "        # we thus have an embedding block of 512 x 10'000 parameters\n",
    "    \n",
    "        # maximum sequence length, though inputs longer than `hidden_size` will be iteratively processed\n",
    "        max_position_embeddings = 512, \n",
    "    \n",
    "        # number of transformer blocks. div by 2 for attention_types\n",
    "        num_layers=2, \n",
    "        # for global and local attention (GPT-Neo-specific)\n",
    "        attention_types=[[[\"global\", \"local\"], 1]], \n",
    "    \n",
    "        num_heads=4,     # attention heads\n",
    "        window_size=256, # for local attention (GPT-Neo-specific)\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_experts=num_experts,\n",
    "        topk=topk,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "    \n",
    "    with open(f\"model_configs/gpt_moe_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_gpt, outfile)\n",
    "\n",
    "    \n",
    "    config_rob = dict(\n",
    "        \n",
    "        # EMBEDDING PARAMETERS\n",
    "        vocab_size              = 10_000,   \n",
    "        hidden_size             = 256,      \n",
    "        # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "        max_position_embeddings = 512 + 1,\n",
    "    \n",
    "        # BLOCKS (of course naming is different in roberta :) )\n",
    "        num_hidden_layers = 2,\n",
    "        num_attention_heads = 4,                   \n",
    "        pad_token_id = 0,\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_experts=num_experts,\n",
    "        topk=topk,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "    with open(f\"model_configs/roberta_moe_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_rob, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cd7c6-f075-48b2-a327-7ca24f8e359a",
   "metadata": {},
   "source": [
    "# CNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85394778-a6e7-4f5d-9216-fc3313ffe592",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = 256\n",
    "intermediate_factor = 16 # for baseline ffn, intermediate size intermediate factor * input dim\n",
    "sparsity_type=SparsityType.CNT\n",
    "sparsity_levels = [('high',16), ('medium',32), ('low',64)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c96ab915-07aa-492f-86a5-6fbf2a21e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate layer size is 3968\n",
      "intermediate layer size is 4032\n",
      "intermediate layer size is 4032\n"
     ]
    }
   ],
   "source": [
    "for pair in sparsity_levels:\n",
    "    st, num_blocks = pair\n",
    "    dim_lowrank = round(dim_in/num_blocks)\n",
    "    intermediate_size = dim_in * (2 * intermediate_factor * num_blocks  - 1) / (2*num_blocks+1) \n",
    "    intermediate_size = num_blocks * round(intermediate_size/ num_blocks) # round to nearest multiple of num blocks\n",
    "    print(\"intermediate layer size is\", intermediate_size)\n",
    "\n",
    "    config_gpt = dict(\n",
    "    \n",
    "        # number of tokens in the vocabulary \n",
    "        vocab_size = 10_000, \n",
    "        # embedding size (vector length) of each token \n",
    "        hidden_size=dim_in, \n",
    "        # we thus have an embedding block of 512 x 10'000 parameters\n",
    "    \n",
    "        # maximum sequence length, though inputs longer than `hidden_size` will be iteratively processed\n",
    "        max_position_embeddings = 512, \n",
    "    \n",
    "        # number of transformer blocks. div by 2 for attention_types\n",
    "        num_layers=2, \n",
    "        # for global and local attention (GPT-Neo-specific)\n",
    "        attention_types=[[[\"global\", \"local\"], 1]], \n",
    "    \n",
    "        num_heads=4,     # attention heads\n",
    "        window_size=256, # for local attention (GPT-Neo-specific)\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_blocks = num_blocks,\n",
    "        dim_lowrank = dim_lowrank,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "    with open(f\"model_configs/gpt_cnt_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_gpt, outfile)\n",
    "\n",
    "    \n",
    "    config_rob = dict(\n",
    "        \n",
    "        # EMBEDDING PARAMETERS\n",
    "        vocab_size              = 10_000,   \n",
    "        hidden_size             = 256,      \n",
    "        # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "        max_position_embeddings = 512 + 1,\n",
    "    \n",
    "        # BLOCKS (of course naming is different in roberta :) )\n",
    "        num_hidden_layers = 2,\n",
    "        num_attention_heads = 4,                   \n",
    "        pad_token_id = 0,\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_blocks = num_blocks,\n",
    "        dim_lowrank = dim_lowrank,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "\n",
    "    with open(f\"model_configs/roberta_cnt_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_rob, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb740c1a-0521-4cd3-8de1-0f3d685c34c8",
   "metadata": {},
   "source": [
    "# PKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061bed93-fe04-4694-93e9-c64a576f7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subkeys 56\n"
     ]
    }
   ],
   "source": [
    "dim_in = 256\n",
    "intermediate_factor = 4\n",
    "intermediate_size = dim_in * intermediate_factor\n",
    "sparsity_type=SparsityType.PKM\n",
    "dim_key = intermediate_size/2\n",
    "num_query_heads = 4\n",
    "num_subkeys = -(num_query_heads * dim_key / dim_in) + ((num_query_heads * dim_key / dim_in)**2 - 2 * (num_query_heads * dim_key - 16 * dim_in) )**0.5\n",
    "num_subkeys = round(num_subkeys)\n",
    "sparsity_levels = [('high', round(0.25 * num_subkeys)), ('medium', round(0.5 * num_subkeys)), ('low', round(0.75 * num_subkeys))]\n",
    "\n",
    "print(\"number of subkeys\", num_subkeys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa74622f-46c9-4011-908f-514e6bd90472",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in sparsity_levels:\n",
    "    st, topk = pair\n",
    "    config_gpt = dict(\n",
    "    \n",
    "        # number of tokens in the vocabulary \n",
    "        vocab_size = 10_000, \n",
    "        # embedding size (vector length) of each token \n",
    "        hidden_size=dim_in, \n",
    "        # we thus have an embedding block of 512 x 10'000 parameters\n",
    "    \n",
    "        # maximum sequence length, though inputs longer than `hidden_size` will be iteratively processed\n",
    "        max_position_embeddings = 512, \n",
    "    \n",
    "        # number of transformer blocks. div by 2 for attention_types\n",
    "        num_layers=2, \n",
    "        # for global and local attention (GPT-Neo-specific)\n",
    "        attention_types=[[[\"global\", \"local\"], 1]], \n",
    "    \n",
    "        num_heads=4,     # attention heads\n",
    "        window_size=256, # for local attention (GPT-Neo-specific)\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_query_heads = num_query_heads,\n",
    "        num_subkeys = num_subkeys,\n",
    "        topk = topk,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "    with open(f\"model_configs/gpt_pkm_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_gpt, outfile)\n",
    "\n",
    "\n",
    "    config_rob = dict(\n",
    "        \n",
    "        # EMBEDDING PARAMETERS\n",
    "        vocab_size              = 10_000,   \n",
    "        hidden_size             = 256,      \n",
    "        # we add 1 as RoBERTa uses a special position embedding for the padding token (zero vector)\n",
    "        max_position_embeddings = 512 + 1,\n",
    "    \n",
    "        # BLOCKS (of course naming is different in roberta :) )\n",
    "        num_hidden_layers = 2,\n",
    "        num_attention_heads = 4,                   \n",
    "        pad_token_id = 0,\n",
    "    \n",
    "        sparsity_type=sparsity_type,\n",
    "        num_query_heads = num_query_heads,\n",
    "        num_subkeys = num_subkeys,\n",
    "        topk = topk,\n",
    "        intermediate_size=intermediate_size, # size of 'up-projection' layer in FFN\n",
    "    )\n",
    "    with open(f\"model_configs/roberta_pkm_{st}.json\", \"w\") as outfile: \n",
    "        json.dump(config_rob, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
