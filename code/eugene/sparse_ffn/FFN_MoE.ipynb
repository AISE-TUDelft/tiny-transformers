{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d15d7ea-0542-4b1a-b11a-4d65d2d953ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e544abd-1204-462b-ab3c-fe6b108fea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, num_experts):\n",
    "        super(Experts, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.num_experts = num_experts\n",
    "        W1 = torch.empty(num_experts, dim_in, dim_hidden)\n",
    "        b1 = torch.empty(num_experts, dim_hidden)\n",
    "        W2 = torch.empty(num_experts, dim_hidden, dim_in)\n",
    "        b2 = torch.empty(num_experts, dim_in)\n",
    "\n",
    "        std = 1/math.sqrt(self.dim_in)\n",
    "        W1.uniform_(-std, std)\n",
    "        b1.uniform_(-std, std)\n",
    "        W2.uniform_(-std, std)\n",
    "        b2.uniform_(-std, std)\n",
    "        \n",
    "        self.W1 = nn.Parameter(W1)\n",
    "        self.b1 = nn.Parameter(b1)\n",
    "        self.W2 = nn.Parameter(W2)\n",
    "        self.b2 = nn.Parameter(b2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x, weights, experts_indices = input_and_weights\n",
    "        #batch, context_length, _ = x.shape\n",
    "        #experts_mask = torch.zeros( (batch, context_length, self.num_experts), device = x.device, dtype = int) # x.shape[:-1] = batch, context_length\n",
    "\n",
    "        #experts_mask.scatter_(-1, experts_indices, torch.ones_like(experts_indices, device = x.device))\n",
    "        a = torch.einsum('bcd,ndh->bcnh', x, self.W1) + self.b1 # pass x to every expert\n",
    "        z = F.relu(a)\n",
    "        y = torch.einsum('bcnh,nhd->bcnd', z, self.W2) + self.b2\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19342d-d566-46da-9d36-6022bd44d1b3",
   "metadata": {},
   "source": [
    "Check if Experts behaves like multiple FFNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1478bea3-81bb-4fcc-b612-77353d266962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7603e-07, grad_fn=<SumBackward0>)\n",
      "tensor(-3.1665e-07, grad_fn=<SumBackward0>)\n",
      "tensor(-8.0094e-07, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# W = torch.rand(3,4,5) # num experts, dim_in, dim_hidden\n",
    "# s = torch.rand(10,7,3) # batch size, context length, num experts (gating vector)\n",
    "# x = torch.rand(10,7,4) # batch size, context length, feature dim (data)\n",
    "# s_v, s_i = torch.topk(s, 2, dim=-1)\n",
    "# b = torch.zeros(10, 7, 3) # batch size, context length, num experts\n",
    "# b.scatter_(-1, s_i, torch.ones_like(s_v))\n",
    "# print(torch.einsum(\"nab,bio->naio\",b,W).shape)\n",
    "\n",
    "e = Experts(10,15,3)\n",
    "data = torch.rand(100,20,10)\n",
    "results = e(data)\n",
    "y0 = F.relu(data @ e.W1[0,:,:] + e.b1[0,:]) @ e.W2[0,:,:] + e.b2[0,:]\n",
    "y1 = F.relu(data @ e.W1[1,:,:] + e.b1[1,:]) @ e.W2[1,:,:] + e.b2[1,:]\n",
    "y2 = F.relu(data @ e.W1[2,:,:] + e.b1[2,:]) @ e.W2[2,:,:] + e.b2[2,:]\n",
    "print(torch.sum(results[:,:,0,:] - y0))\n",
    "print(torch.sum(results[:,:,1,:] - y1))\n",
    "print(torch.sum(results[:,:,2,:] - y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96dfee0b-7fa5-4836-aae5-91787514e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, num_experts, top_k, utilization_factor = 1e-2):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.Wg = nn.Linear(dim_in, num_experts, bias = False)\n",
    "        self.Wnoise = nn.Linear(dim_in, num_experts, bias = False)\n",
    "        self.utilization_factor = utilization_factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        noise = F.softplus(self.Wnoise(x))\n",
    "        noise *= torch.randn_like(noise).to(x.device)\n",
    "        logits = self.Wg(x)\n",
    "        logits += noise\n",
    "        mask = torch.full_like(logits, -float('inf')).to(x.device)\n",
    "        selected_logits, selected_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        mask.scatter_(-1,selected_indices, selected_logits)\n",
    "        weights = F.softmax(mask, dim=-1)\n",
    "        return weights, self.utilization_loss(weights)\n",
    "\n",
    "    def utilization_loss(self, weights):\n",
    "        importance = weights.reshape(-1, self.num_experts).sum(dim=0)\n",
    "        square_cv = importance.var(correction=0) / importance.mean().pow(2)\n",
    "        return self.utilization_factor * square_cv\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea5c630e-7b2f-408e-97dc-103f14e82858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, num_experts, topk):\n",
    "        super(MoE, self).__init__()\n",
    "        # no need for dropout because it's already sparse?\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.gating = GatingNetwork(dim_in, num_experts, topk)\n",
    "        self.experts = Experts(dim_in, dim_hidden, num_experts)\n",
    "    def forward(self, x):\n",
    "        weights, loss = self.gating(x)\n",
    "        expert_results = self.experts(x)\n",
    "        return torch.einsum('bcn,bcnd->bcd', weights, expert_results), loss\n",
    "        # this implementation probably activates all the parameters, so no computational speed up. But that's not important for this RQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fffe6110-1500-4336-8c3e-312593052d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6])\n",
      "tensor(0.0119, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(1,1,6)\n",
    "m = MoE(6,12,4,2)\n",
    "print(m(data)[0].shape)\n",
    "print(m(data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc794388-9e5b-4dec-b474-c7425173521d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3238,  0.2572,  0.0399, -0.4033,  0.1174, -0.6530])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7804 * torch.tensor([ 0.3652,  0.3562,  0.1133, -0.4165,  0.2291, -0.7671]) + 0.2196 * torch.tensor([ 0.1765, -0.0946, -0.2208, -0.3564, -0.2796, -0.2473])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a1a4c-8808-4051-b1a5-a2a64720cf61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
